{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WOx5RPJRg9Ha",
        "outputId": "323c1057-2413-49b2-b0d1-eaed7db71f53"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "cuda\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from copy import deepcopy\n",
        "import time\n",
        "import torch.nn.functional as F\n",
        "from sklearn.cluster import KMeans\n",
        "import os\n",
        "import json\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"{device}\")\n",
        "\n",
        "torch.manual_seed(1212)\n",
        "np.random.seed(1212)\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.manual_seed(1212)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UUW7_oephSUW",
        "outputId": "6aa47c89-62ce-4398-914a-7d7f787df0db"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "# of parameter: 15,253,578\n"
          ]
        }
      ],
      "source": [
        "class VGG(nn.Module):\n",
        "    def __init__(self, vgg_name='VGG16', num_classes=10):\n",
        "        super(VGG, self).__init__()\n",
        "        self.cfg = {\n",
        "            'VGG11': [64, 'M', 128, 'M', 256, 256, 'M', 512, 512, 'M', 512, 512, 'M'],\n",
        "            'VGG13': [64, 64, 'M', 128, 128, 'M', 256, 256, 'M', 512, 512, 'M', 512, 512, 'M'],\n",
        "            'VGG16': [64, 64, 'M', 128, 128, 'M', 256, 256, 256, 'M', 512, 512, 512, 'M', 512, 512, 512, 'M'],\n",
        "            'VGG19': [64, 64, 'M', 128, 128, 'M', 256, 256, 256, 256, 'M', 512, 512, 512, 512, 'M', 512, 512, 512, 512, 'M'],\n",
        "        }\n",
        "        self.features = self._make_layers(self.cfg[vgg_name])\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Linear(512, 512),\n",
        "            nn.ReLU(True),\n",
        "            nn.Dropout(),\n",
        "            nn.Linear(512, 512),\n",
        "            nn.ReLU(True),\n",
        "            nn.Dropout(),\n",
        "            nn.Linear(512, num_classes)\n",
        "        )\n",
        "\n",
        "    def _make_layers(self, cfg):\n",
        "        layers = []\n",
        "        in_channels = 3\n",
        "        for x in cfg:\n",
        "            if x == 'M':\n",
        "                layers += [nn.MaxPool2d(kernel_size=2, stride=2)]\n",
        "            else:\n",
        "                layers += [nn.Conv2d(in_channels, x, kernel_size=3, padding=1),\n",
        "                          nn.BatchNorm2d(x),\n",
        "                          nn.ReLU(inplace=True)]\n",
        "                in_channels = x\n",
        "        layers += [nn.AvgPool2d(kernel_size=1, stride=1)]\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.features(x)\n",
        "        out = out.view(out.size(0), -1)\n",
        "        out = self.classifier(out)\n",
        "        return out\n",
        "\n",
        "model = VGG('VGG16').to(device)\n",
        "print(f\"# of parameter: {sum(p.numel() for p in model.parameters()):,}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KCHS-w8UhTEA",
        "outputId": "8a31533b-9309-4566-c255-ac23848fd883"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 170M/170M [00:05<00:00, 30.8MB/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Size of training set: 50000\n",
            "Size of testing set: 10000\n",
            "Size fo score subset: 1000\n"
          ]
        }
      ],
      "source": [
        "transform_train = transforms.Compose([\n",
        "    transforms.RandomCrop(32, padding=4),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
        "])\n",
        "\n",
        "transform_test = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
        "])\n",
        "\n",
        "trainset = torchvision.datasets.CIFAR10(\n",
        "    root='./data', train=True, download=True, transform=transform_train)\n",
        "trainloader = torch.utils.data.DataLoader(\n",
        "    trainset, batch_size=128, shuffle=True, num_workers=2)\n",
        "\n",
        "testset = torchvision.datasets.CIFAR10(\n",
        "    root='./data', train=False, download=True, transform=transform_test)\n",
        "testloader = torch.utils.data.DataLoader(\n",
        "    testset, batch_size=100, shuffle=False, num_workers=2)\n",
        "\n",
        "score_trainset = torch.utils.data.Subset(trainset, range(0, len(trainset), 50))  # 2% of dataset\n",
        "score_loader = torch.utils.data.DataLoader(\n",
        "    score_trainset, batch_size=128, shuffle=True, num_workers=2)\n",
        "\n",
        "classes = ('plane', 'car', 'bird', 'cat', 'deer',\n",
        "           'dog', 'frog', 'horse', 'ship', 'truck')\n",
        "\n",
        "print(f\"Size of training set: {len(trainset)}\")\n",
        "print(f\"Size of testing set: {len(testset)}\")\n",
        "print(f\"Size fo score subset: {len(score_trainset)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UigYSu_DRPNW"
      },
      "source": [
        "# Noise data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "miOWSP8oSfvT"
      },
      "outputs": [],
      "source": [
        "!git clone https://github.com/UCSC-REAL/cifar-10-100n.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xbnwELb7RRFg"
      },
      "outputs": [],
      "source": [
        "def apply_symmetric_label_noise(labels, noise_rate, num_classes=10, seed=None):\n",
        "    labels = labels.clone().long()\n",
        "    N = labels.size(0)\n",
        "\n",
        "    g = torch.Generator()\n",
        "    if seed is not None:\n",
        "        g.manual_seed(seed)\n",
        "\n",
        "    flip_mask = torch.rand(N, generator=g) < noise_rate\n",
        "    num_flip = flip_mask.sum()\n",
        "\n",
        "    if num_flip == 0:\n",
        "        return labels, flip_mask\n",
        "    new_labels = torch.randint(low=0, high=num_classes, size=(num_flip,), generator=g)\n",
        "    orig = labels[flip_mask]\n",
        "    same = new_labels == orig\n",
        "    while same.any():\n",
        "        new_labels[same] = torch.randint(0, num_classes, (same.sum(),), generator=g)\n",
        "        same = new_labels == orig\n",
        "\n",
        "    labels[flip_mask] = new_labels\n",
        "    return labels, flip_mask\n",
        "\n",
        "class CIFAR10NWithNoise(Dataset):\n",
        "    def __init__(self, base_dataset, labels_tensor):\n",
        "        self.base = base_dataset\n",
        "        self.labels = labels_tensor.long()\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.base)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img, _ = self.base[idx]\n",
        "        label = self.labels[idx].item()\n",
        "        return img, label\n",
        "\n",
        "noise_file = torch.load(\"./cifar-10-100n/data/CIFAR-10_human.pt\", weights_only=False)\n",
        "\n",
        "clean_label  = torch.as_tensor(noise_file['clean_label']).long()\n",
        "worst_label  = torch.as_tensor(noise_file['worse_label']).long()\n",
        "aggre_label  = torch.as_tensor(noise_file['aggre_label']).long()\n",
        "# random_label1 = noise_file[\"random_label1\"]\n",
        "# random_label2 = noise_file[\"random_label2\"]\n",
        "# random_label3 = noise_file[\"random_label3\"]\n",
        "\n",
        "noise_rates = [0.1, 0.3, 0.5]\n",
        "noisy_labels_dict = {}\n",
        "\n",
        "for rate in noise_rates:\n",
        "    noisy_labels, flip_mask = apply_symmetric_label_noise(\n",
        "        clean_label,\n",
        "        noise_rate=rate,\n",
        "        num_classes=10,\n",
        "        seed=42\n",
        "    )\n",
        "    noisy_labels_dict[rate] = noisy_labels\n",
        "    print(f\"noise rate {rate*100:.0f}%: {flip_mask.sum().item()} / {len(clean_label)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ei40RJSvR1m9"
      },
      "outputs": [],
      "source": [
        "train_clean = CIFAR10NWithNoise(trainset, clean_label)\n",
        "# 10% noise\n",
        "train_noise_10 = CIFAR10NWithNoise(trainset, noisy_labels_dict[0.1])\n",
        "# 30% noise\n",
        "train_noise_30 = CIFAR10NWithNoise(trainset, noisy_labels_dict[0.3])\n",
        "# 50% noise\n",
        "train_noise_50 = CIFAR10NWithNoise(trainset, noisy_labels_dict[0.5])\n",
        "\n",
        "# EX: 30% noise：\n",
        "# trainloader = DataLoader(train_noise_30, batch_size=128, shuffle=True, num_workers=4)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5ljQ5eI-efR3"
      },
      "source": [
        "# checkpoint"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LEL_TWK6ehNy"
      },
      "outputs": [],
      "source": [
        "def save_checkpoint(model, results, optimizer, epoch, filepath, model_type='original'):\n",
        "    \"\"\"\n",
        "    Save model checkpoint\n",
        "\n",
        "    Args:\n",
        "        model: Model\n",
        "        results: Training results dictionary\n",
        "        optimizer: Optimizer\n",
        "        epoch: Current epoch\n",
        "        filepath: Save path\n",
        "        model_type: Model type ('original', 'snip', 'random')\n",
        "    \"\"\"\n",
        "    checkpoint = {\n",
        "        'model_state_dict': model.state_dict(),\n",
        "        'optimizer_state_dict': optimizer.state_dict(),\n",
        "        'epoch': epoch,\n",
        "        'results': results,\n",
        "        'model_type': model_type\n",
        "    }\n",
        "    torch.save(checkpoint, filepath)\n",
        "    print(f\"Checkpoint saved to {filepath}\")\n",
        "\n",
        "\n",
        "def load_checkpoint(model, optimizer, filepath):\n",
        "    \"\"\"\n",
        "    Load model checkpoint\n",
        "\n",
        "    Returns:\n",
        "        model, optimizer, epoch, results, model_type\n",
        "    \"\"\"\n",
        "    if not os.path.exists(filepath):\n",
        "        print(f\"Checkpoint not found: {filepath}\")\n",
        "        return model, optimizer, 0, None, None\n",
        "\n",
        "\n",
        "    checkpoint = torch.load(filepath, map_location=device)\n",
        "    model.load_state_dict(checkpoint['model_state_dict'])\n",
        "    if optimizer is not None:\n",
        "        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "\n",
        "    print(f\"Checkpoint loaded from {filepath}\")\n",
        "    return model, optimizer, checkpoint['epoch'], checkpoint['results'], checkpoint['model_type']\n",
        "\n",
        "\n",
        "def save_results(results, filepath='results.json'):\n",
        "    \"\"\"Save training results as JSON\"\"\"\n",
        "    with open(filepath, 'w') as f:\n",
        "        json.dump(results, f, indent=4)\n",
        "    print(f\"Results saved to {filepath}\")\n",
        "\n",
        "\n",
        "def load_results(filepath='results.json'):\n",
        "    \"\"\"Load training results\"\"\"\n",
        "    if not os.path.exists(filepath):\n",
        "        print(f\"Results file not found: {filepath}\")\n",
        "        return None\n",
        "\n",
        "    with open(filepath, 'r') as f:\n",
        "        results = json.load(f)\n",
        "    print(f\"Results loaded from {filepath}\")\n",
        "    return results\n",
        "\n",
        "\n",
        "def save_pruning_masks(keep_masks, filepath='pruning_masks.pth'):\n",
        "    \"\"\"Save pruning masks\"\"\"\n",
        "    torch.save(keep_masks, filepath)\n",
        "    print(f\"Pruning masks saved to {filepath}\")\n",
        "\n",
        "\n",
        "def load_pruning_masks(filepath='pruning_masks.pth'):\n",
        "    \"\"\"Load pruning masks\"\"\"\n",
        "    if not os.path.exists(filepath):\n",
        "        print(f\"Pruning masks not found: {filepath}\")\n",
        "        return None\n",
        "\n",
        "    keep_masks = torch.load(filepath, map_location=device)\n",
        "    print(f\"Pruning masks loaded from {filepath}\")\n",
        "    return keep_masks\n",
        "\n",
        "def save_masks(mask_data, filepath):\n",
        "    \"\"\"Save pruning masks\"\"\"\n",
        "    torch.save(mask_data, filepath)\n",
        "    print(f\"✓ Masks saved: {filepath}\")\n",
        "\n",
        "\n",
        "def load_masks(filepath):\n",
        "    \"\"\"Load pruning masks\"\"\"\n",
        "    if not os.path.exists(filepath):\n",
        "        print(f\"No masks found: {filepath}\")\n",
        "        return None\n",
        "\n",
        "    masks = torch.load(filepath, map_location=device)\n",
        "    print(f\"✓ Masks loaded: {filepath}\")\n",
        "    return masks"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FDemMsyReR7r"
      },
      "source": [
        "# SNIP Function\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rRcef_CJeUJ4",
        "outputId": "1bdbb91a-d0bf-477d-982b-64c88e0080dd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "SNIP 演算法實作完成！\n"
          ]
        }
      ],
      "source": [
        "def snip_forward_conv2d(self, x):\n",
        "    \"\"\"Custom Conv2d forward for computing connection sensitivity\"\"\"\n",
        "    return nn.functional.conv2d(x, self.weight * self.weight_mask, self.bias,\n",
        "                                self.stride, self.padding, self.dilation, self.groups)\n",
        "\n",
        "def snip_forward_linear(self, x):\n",
        "    \"\"\"Custom Linear forward for computing connection sensitivity\"\"\"\n",
        "    return nn.functional.linear(x, self.weight * self.weight_mask, self.bias)\n",
        "\n",
        "def apply_prune_mask(model, keep_masks):\n",
        "    \"\"\"Apply pruning masks to model weights\"\"\"\n",
        "    prunable_layers = filter(\n",
        "        lambda layer: isinstance(layer, nn.Conv2d) or isinstance(layer, nn.Linear),\n",
        "        model.modules())\n",
        "\n",
        "    for layer, keep_mask in zip(prunable_layers, keep_masks):\n",
        "        assert (layer.weight.shape == keep_mask.shape)\n",
        "\n",
        "        def hook_factory(keep_mask):\n",
        "            def hook(grads):\n",
        "                return grads * keep_mask\n",
        "            return hook\n",
        "\n",
        "        # Register hook to ensure pruned weights have zero gradients\n",
        "        layer.weight.data[keep_mask == 0.] = 0.\n",
        "        layer.weight.register_hook(hook_factory(keep_mask))\n",
        "\n",
        "def snip_pruning(model, dataloader, sparsity=0.5):\n",
        "    \"\"\"\n",
        "    Perform SNIP pruning\n",
        "\n",
        "    Args:\n",
        "        model: Model to prune\n",
        "        dataloader: Data loader for computing connection sensitivity\n",
        "        sparsity: Pruning rate (0.5 means prune 50% of parameters)\n",
        "\n",
        "    Returns:\n",
        "        keep_masks: Keep masks for each layer\n",
        "    \"\"\"\n",
        "    # Save original requires_grad state\n",
        "    original_requires_grad = {}\n",
        "\n",
        "    # Replace forward methods of all prunable layers with custom versions\n",
        "    for name, layer in model.named_modules():\n",
        "        if isinstance(layer, nn.Conv2d):\n",
        "            original_requires_grad[name] = layer.weight.requires_grad\n",
        "            layer.weight_mask = nn.Parameter(torch.ones_like(layer.weight))\n",
        "            layer.weight.requires_grad = False\n",
        "        elif isinstance(layer, nn.Linear):\n",
        "            original_requires_grad[name] = layer.weight.requires_grad\n",
        "            layer.weight_mask = nn.Parameter(torch.ones_like(layer.weight))\n",
        "            layer.weight.requires_grad = False\n",
        "\n",
        "    # Replace forward methods\n",
        "    for layer in model.modules():\n",
        "        if isinstance(layer, nn.Conv2d):\n",
        "            layer.forward = snip_forward_conv2d.__get__(layer, nn.Conv2d)\n",
        "        elif isinstance(layer, nn.Linear):\n",
        "            layer.forward = snip_forward_linear.__get__(layer, nn.Linear)\n",
        "\n",
        "    # Compute connection sensitivity\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    inputs, targets = next(iter(dataloader))\n",
        "    inputs, targets = inputs.to(device), targets.to(device)\n",
        "\n",
        "    outputs = model(inputs)\n",
        "    loss = criterion(outputs, targets)\n",
        "    loss.backward()\n",
        "\n",
        "    # Collect gradients (connection sensitivity) from all layers\n",
        "    grads_abs = []\n",
        "    for layer in model.modules():\n",
        "        if isinstance(layer, nn.Conv2d) or isinstance(layer, nn.Linear):\n",
        "            grads_abs.append(torch.abs(layer.weight_mask.grad))\n",
        "\n",
        "    # Concatenate all gradients into a 1D vector\n",
        "    all_scores = torch.cat([torch.flatten(x) for x in grads_abs])\n",
        "\n",
        "    # Compute threshold\n",
        "    num_params_to_keep = int(len(all_scores) * (1 - sparsity))\n",
        "    threshold, _ = torch.kthvalue(all_scores, len(all_scores) - num_params_to_keep)\n",
        "\n",
        "    # Generate keep masks for each layer\n",
        "    keep_masks = []\n",
        "    for g in grads_abs:\n",
        "        keep_masks.append((g >= threshold).float())\n",
        "\n",
        "    print(f\"SNIP pruning completed! Pruning rate: {sparsity * 100:.1f}%\")\n",
        "    print(f\"Threshold: {threshold:.6f}\")\n",
        "\n",
        "    # Restore original forward methods and requires_grad states\n",
        "    for name, layer in model.named_modules():\n",
        "        if isinstance(layer, nn.Conv2d):\n",
        "            layer.forward = nn.Conv2d.forward.__get__(layer, nn.Conv2d)\n",
        "            layer.weight.requires_grad = original_requires_grad[name]\n",
        "            del layer.weight_mask\n",
        "        elif isinstance(layer, nn.Linear):\n",
        "            layer.forward = nn.Linear.forward.__get__(layer, nn.Linear)\n",
        "            layer.weight.requires_grad = original_requires_grad[name]\n",
        "            del layer.weight_mask\n",
        "\n",
        "    return keep_masks\n",
        "\n",
        "print(\"SNIP algorithm implementation completed!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wGyoZSv9eXOO"
      },
      "source": [
        "# Random Function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TtCRw8XBeamm"
      },
      "outputs": [],
      "source": [
        "def random_pruning(model, sparsity=0.5):\n",
        "    \"\"\"\n",
        "    Perform random pruning - randomly select weights to keep\n",
        "\n",
        "    Args:\n",
        "        model: Model to prune\n",
        "        sparsity: Pruning rate (0.5 means prune 50% of parameters)\n",
        "\n",
        "    Returns:\n",
        "        keep_masks: Keep masks for each layer\n",
        "    \"\"\"\n",
        "    print(f\"Performing random pruning (pruning rate: {sparsity * 100:.1f}%)...\")\n",
        "\n",
        "    keep_masks = []\n",
        "    total_params = 0\n",
        "    kept_params = 0\n",
        "\n",
        "    # Collect weights from all prunable layers\n",
        "    prunable_layers = []\n",
        "    for layer in model.modules():\n",
        "        if isinstance(layer, nn.Conv2d) or isinstance(layer, nn.Linear):\n",
        "            prunable_layers.append(layer)\n",
        "\n",
        "    # Generate random masks for each layer\n",
        "    for layer in prunable_layers:\n",
        "        weight_shape = layer.weight.shape\n",
        "        total_elements = layer.weight.numel()\n",
        "\n",
        "        # Generate random mask\n",
        "        flat_mask = torch.ones(total_elements)\n",
        "        num_to_prune = int(total_elements * sparsity)\n",
        "\n",
        "        # Randomly select positions to prune\n",
        "        prune_indices = torch.randperm(total_elements)[:num_to_prune]\n",
        "        flat_mask[prune_indices] = 0\n",
        "\n",
        "        # Reshape to original shape\n",
        "        keep_mask = flat_mask.view(weight_shape).to(device)\n",
        "        keep_masks.append(keep_mask)\n",
        "\n",
        "        total_params += total_elements\n",
        "        kept_params += (keep_mask == 1).sum().item()\n",
        "\n",
        "    actual_sparsity = 1 - (kept_params / total_params)\n",
        "    print(f\"Random pruning completed!\")\n",
        "    print(f\"Actual pruning rate: {actual_sparsity * 100:.2f}%\")\n",
        "    print(f\"Parameters kept: {kept_params:,} / {total_params:,}\")\n",
        "\n",
        "    return keep_masks"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VrvOd6R0p9Ql"
      },
      "source": [
        "# GSRP structured (Filter-level) Pruning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wpgVSinb750h"
      },
      "outputs": [],
      "source": [
        "def get_conv_layers(model):\n",
        "    conv_infos = []\n",
        "    for name, module in model.named_modules():\n",
        "        if isinstance(module, nn.Conv2d):\n",
        "            conv_infos.append({\n",
        "                \"name\": name,\n",
        "                \"layer\": module,\n",
        "                \"out_channels\": module.out_channels\n",
        "            })\n",
        "    return conv_infos\n",
        "\n",
        "def compute_gsrp_scores(model, loader, device, T=3):\n",
        "    model.eval()\n",
        "    conv_infos = get_conv_layers(model)\n",
        "\n",
        "    for info in conv_infos:\n",
        "        n_filters = info[\"out_channels\"]\n",
        "        info[\"signs\"] = torch.zeros(n_filters, T, dtype=torch.int8)\n",
        "        info[\"snip\"] = torch.zeros(n_filters, dtype=torch.float32)\n",
        "\n",
        "    data_iter = iter(loader)\n",
        "\n",
        "    for t in range(T):\n",
        "        try:\n",
        "            x, y = next(data_iter)\n",
        "        except StopIteration:\n",
        "            data_iter = iter(loader)\n",
        "            x, y = next(data_iter)\n",
        "\n",
        "        x, y = x.to(device), y.to(device)\n",
        "        for p in model.parameters():\n",
        "            if p.grad is not None:\n",
        "                p.grad.zero_()\n",
        "\n",
        "        logits = model(x)\n",
        "        loss = F.cross_entropy(logits, y)\n",
        "        loss.backward()\n",
        "\n",
        "        for info in conv_infos:\n",
        "            layer = info[\"layer\"]\n",
        "            g = layer.weight.grad.detach()\n",
        "            w = layer.weight.detach()\n",
        "\n",
        "            g_scalar = g.view(g.size(0), -1).mean(dim=1)\n",
        "            sign = torch.sign(g_scalar)\n",
        "            sign[sign == 0] = 1\n",
        "            info[\"signs\"][:, t] = sign.cpu().to(torch.int8)\n",
        "\n",
        "            snip = (g * w).abs().view(g.size(0), -1).sum(dim=1)\n",
        "            info[\"snip\"] += snip.cpu()\n",
        "\n",
        "    for info in conv_infos:\n",
        "        signs = info[\"signs\"].float()\n",
        "        majority = torch.sign(signs.sum(dim=1))\n",
        "        majority[majority == 0] = 1\n",
        "        agree = (signs == majority.unsqueeze(1)).float().mean(dim=1)\n",
        "        C = agree\n",
        "\n",
        "        B = info[\"snip\"]\n",
        "        if B.max() > 0:\n",
        "            B = B / B.max()\n",
        "\n",
        "        S = C * B\n",
        "        info[\"C\"] = C\n",
        "        info[\"B\"] = B\n",
        "        info[\"S\"] = S\n",
        "\n",
        "    return conv_infos\n",
        "\n",
        "def collect_conv_activations(model, loader, device, conv_infos, num_batches=10):\n",
        "    model.eval()\n",
        "\n",
        "    for info in conv_infos:\n",
        "        F = info[\"out_channels\"]\n",
        "        info[\"act_sum\"] = torch.zeros(F)\n",
        "        info[\"act_count\"] = 0\n",
        "\n",
        "    handles = []\n",
        "    for info in conv_infos:\n",
        "        layer = info[\"layer\"]\n",
        "\n",
        "        def make_hook(info_ref):\n",
        "            def hook(module, inp, out):\n",
        "                act = out.detach().mean(dim=(0, 2, 3)).cpu()\n",
        "                info_ref[\"act_sum\"] += act\n",
        "                info_ref[\"act_count\"] += 1\n",
        "            return hook\n",
        "\n",
        "        h = layer.register_forward_hook(make_hook(info))\n",
        "        handles.append(h)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for i, (x, _) in enumerate(loader):\n",
        "            if i >= num_batches:\n",
        "                break\n",
        "            x = x.to(device)\n",
        "            model(x)\n",
        "\n",
        "    for h in handles:\n",
        "        h.remove()\n",
        "\n",
        "    for info in conv_infos:\n",
        "        if info[\"act_count\"] > 0:\n",
        "            info[\"acts\"] = info[\"act_sum\"] / info[\"act_count\"]\n",
        "        else:\n",
        "            info[\"acts\"] = info[\"act_sum\"]\n",
        "\n",
        "    return conv_infos\n",
        "\n",
        "def choose_filters_by_gsrp(conv_infos, target_sparsity=0.9, use_clustering=True):\n",
        "    for info in conv_infos:\n",
        "        F = info[\"out_channels\"]\n",
        "        keep_k = max(1, int(F * (1 - target_sparsity)))\n",
        "        S = info[\"S\"]\n",
        "\n",
        "        if not use_clustering or keep_k >= F:\n",
        "            _, idx = torch.topk(S, k=keep_k)\n",
        "            mask = torch.zeros(F, dtype=torch.bool)\n",
        "            mask[idx] = True\n",
        "            info[\"mask\"] = mask\n",
        "            continue\n",
        "\n",
        "        acts = info[\"acts\"].view(-1, 1).numpy()\n",
        "        K = min(keep_k, F)\n",
        "\n",
        "        if K <= 1:\n",
        "            idx = torch.argmax(S).unsqueeze(0)\n",
        "            mask = torch.zeros(F, dtype=torch.bool)\n",
        "            mask[idx] = True\n",
        "            info[\"mask\"] = mask\n",
        "            continue\n",
        "\n",
        "        kmeans = KMeans(n_clusters=K, n_init=10, random_state=12)\n",
        "        labels = kmeans.fit_predict(acts)\n",
        "\n",
        "        S_np = S.numpy()\n",
        "        keep_idx = []\n",
        "        for c in range(K):\n",
        "            members = np.where(labels == c)[0]\n",
        "            if len(members) == 0:\n",
        "                continue\n",
        "            best = members[S_np[members].argmax()]\n",
        "            keep_idx.append(best)\n",
        "\n",
        "        keep_idx = list(sorted(set(keep_idx)))\n",
        "\n",
        "        if len(keep_idx) < keep_k:\n",
        "            remaining = [i for i in range(F) if i not in keep_idx]\n",
        "            extra_needed = keep_k - len(keep_idx)\n",
        "            extra = sorted(remaining, key=lambda i: -S_np[i])[:extra_needed]\n",
        "            keep_idx.extend(extra)\n",
        "\n",
        "        mask = torch.zeros(F, dtype=torch.bool)\n",
        "        mask[keep_idx] = True\n",
        "        info[\"mask\"] = mask\n",
        "\n",
        "    return conv_infos\n",
        "\n",
        "@torch.no_grad()\n",
        "def apply_masks(conv_infos):\n",
        "    for info in conv_infos:\n",
        "        mask = info.get(\"mask\", None)\n",
        "        if mask is None:\n",
        "            continue\n",
        "        layer = info[\"layer\"]\n",
        "        w = layer.weight\n",
        "        pruned = ~mask\n",
        "        w[pruned, :, :, :] = 0.0\n",
        "        if layer.bias is not None:\n",
        "            layer.bias[pruned] = 0.0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pPTLsmzP9QFy"
      },
      "source": [
        "# GSRP weight-wise (unstructure) pruning\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v08blMex9Pw4"
      },
      "outputs": [],
      "source": [
        "\n",
        "def get_prunable_params(model):\n",
        "    param_infos = []\n",
        "    for name, module in model.named_modules():\n",
        "        if isinstance(module, (nn.Conv2d, nn.Linear)):\n",
        "            param_infos.append({\n",
        "                \"name\": name,\n",
        "                \"module\": module,\n",
        "                \"weight\": module.weight,\n",
        "                \"shape\": module.weight.shape,\n",
        "            })\n",
        "    return param_infos\n",
        "\n",
        "def compute_gsrp_scores_weightwise(model, loader, device, T=3):\n",
        "    model.eval()\n",
        "    param_infos = get_prunable_params(model)\n",
        "\n",
        "    for info in param_infos:\n",
        "        shape = info[\"shape\"]\n",
        "        info[\"sign_sum\"] = torch.zeros(shape, dtype=torch.float32, device=device)\n",
        "        info[\"snip\"] = torch.zeros(shape, dtype=torch.float32, device=device)\n",
        "\n",
        "    data_iter = iter(loader)\n",
        "\n",
        "    for t in range(T):\n",
        "        try:\n",
        "            x, y = next(data_iter)\n",
        "        except StopIteration:\n",
        "            data_iter = iter(loader)\n",
        "            x, y = next(data_iter)\n",
        "\n",
        "        x, y = x.to(device), y.to(device)\n",
        "        model.zero_grad(set_to_none=True)\n",
        "\n",
        "        logits = model(x)\n",
        "        loss = F.cross_entropy(logits, y)\n",
        "        loss.backward()\n",
        "\n",
        "        for info in param_infos:\n",
        "            w = info[\"weight\"]\n",
        "            g = w.grad.detach()\n",
        "\n",
        "            sign = torch.sign(g)\n",
        "            sign[sign == 0] = 1.0\n",
        "            info[\"sign_sum\"] += sign\n",
        "\n",
        "            snip = (g * w).abs()\n",
        "            info[\"snip\"] += snip\n",
        "\n",
        "    for info in param_infos:\n",
        "        sign_sum = info[\"sign_sum\"]\n",
        "        snip_sum = info[\"snip\"]\n",
        "\n",
        "        C = (T + sign_sum.abs()) / (2.0 * T)\n",
        "\n",
        "        B = snip_sum\n",
        "        maxB = B.max()\n",
        "        if maxB > 0:\n",
        "            B = B / maxB\n",
        "\n",
        "        S = C * B\n",
        "\n",
        "        info[\"C\"] = C\n",
        "        info[\"B\"] = B\n",
        "        info[\"S\"] = S\n",
        "\n",
        "    return param_infos\n",
        "\n",
        "def choose_weights_by_gsrp_weightwise(param_infos, target_sparsity=0.9):\n",
        "    all_scores = []\n",
        "    for info in param_infos:\n",
        "        all_scores.append(info[\"S\"].view(-1))\n",
        "    all_scores_flat = torch.cat(all_scores)\n",
        "\n",
        "    N = all_scores_flat.numel()\n",
        "    keep_k = max(1, int(N * (1.0 - target_sparsity)))\n",
        "\n",
        "    if keep_k >= N:\n",
        "        thresh = all_scores_flat.min() - 1.0\n",
        "    else:\n",
        "        topk_vals, _ = torch.topk(all_scores_flat, keep_k, largest=True)\n",
        "        thresh = topk_vals[-1].item()\n",
        "\n",
        "    for info in param_infos:\n",
        "        S = info[\"S\"]\n",
        "        mask = (S >= thresh)\n",
        "        info[\"mask\"] = mask\n",
        "\n",
        "    return param_infos\n",
        "\n",
        "@torch.no_grad()\n",
        "def apply_weightwise_masks(param_infos):\n",
        "    for info in param_infos:\n",
        "        mask = info.get(\"mask\", None)\n",
        "        if mask is None:\n",
        "            continue\n",
        "        w = info[\"weight\"]\n",
        "        m = mask.to(w.device).to(w.dtype)\n",
        "        w.mul_(m)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "feGBXc3C9i1m"
      },
      "source": [
        "# Structure pruning (for method 3&4)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CZIEXBoF9JMu"
      },
      "outputs": [],
      "source": [
        "\n",
        "def prune_vgg_model_physically(original_model, gsrp_infos):\n",
        "    \"\"\"\n",
        "    Create a new VGG model with filters physically removed.\n",
        "    This creates a smaller, faster model.\n",
        "    \"\"\"\n",
        "    from copy import deepcopy\n",
        "\n",
        "    conv_masks = {}\n",
        "    for info in gsrp_infos:\n",
        "        conv_masks[info['name']] = info['mask']\n",
        "\n",
        "    new_features = []\n",
        "    prev_out_channels = 3\n",
        "\n",
        "    for module in original_model.features:\n",
        "        if isinstance(module, nn.Conv2d):\n",
        "            conv_name = None\n",
        "            for name, m in original_model.named_modules():\n",
        "                if m is module:\n",
        "                    conv_name = name\n",
        "                    break\n",
        "\n",
        "            if conv_name in conv_masks:\n",
        "                keep_mask = conv_masks[conv_name]\n",
        "                keep_indices = torch.where(keep_mask)[0]\n",
        "            else:\n",
        "                keep_indices = torch.arange(module.out_channels)\n",
        "\n",
        "            new_conv = nn.Conv2d(\n",
        "                in_channels=prev_out_channels,\n",
        "                out_channels=len(keep_indices),\n",
        "                kernel_size=module.kernel_size,\n",
        "                stride=module.stride,\n",
        "                padding=module.padding,\n",
        "                dilation=module.dilation,\n",
        "                groups=module.groups,\n",
        "                bias=(module.bias is not None)\n",
        "            )\n",
        "\n",
        "            if prev_out_channels == module.in_channels:\n",
        "                new_conv.weight.data = module.weight.data[keep_indices].clone()\n",
        "            else:\n",
        "                new_conv.weight.data = module.weight.data[keep_indices][:, :prev_out_channels, :, :].clone()\n",
        "\n",
        "            if module.bias is not None:\n",
        "                new_conv.bias.data = module.bias.data[keep_indices].clone()\n",
        "\n",
        "            new_features.append(new_conv)\n",
        "            prev_out_channels = len(keep_indices)\n",
        "\n",
        "        elif isinstance(module, nn.BatchNorm2d):\n",
        "            new_bn = nn.BatchNorm2d(prev_out_channels)\n",
        "\n",
        "            if prev_out_channels <= module.num_features:\n",
        "                new_bn.weight.data = module.weight.data[:prev_out_channels].clone()\n",
        "                new_bn.bias.data = module.bias.data[:prev_out_channels].clone()\n",
        "                new_bn.running_mean = module.running_mean[:prev_out_channels].clone()\n",
        "                new_bn.running_var = module.running_var[:prev_out_channels].clone()\n",
        "\n",
        "            new_bn.num_batches_tracked = module.num_batches_tracked.clone()\n",
        "            new_features.append(new_bn)\n",
        "\n",
        "        else:\n",
        "            new_features.append(deepcopy(module))\n",
        "\n",
        "    # Build new classifier\n",
        "    # After all pooling (32->16->8->4->2->1) for CIFAR-10, spatial size is 1x1\n",
        "    # So input features = prev_out_channels * 1 * 1\n",
        "    new_first_linear_in = prev_out_channels\n",
        "\n",
        "    new_classifier = nn.Sequential(\n",
        "        nn.Linear(new_first_linear_in, 512),\n",
        "        nn.ReLU(True),\n",
        "        nn.Dropout(),\n",
        "        nn.Linear(512, 512),\n",
        "        nn.ReLU(True),\n",
        "        nn.Dropout(),\n",
        "        nn.Linear(512, 10)\n",
        "    )\n",
        "\n",
        "    orig_first_linear = original_model.classifier[0]\n",
        "    if new_first_linear_in <= orig_first_linear.in_features:\n",
        "        new_classifier[0].weight.data = orig_first_linear.weight.data[:, :new_first_linear_in].clone()\n",
        "    else:\n",
        "        new_classifier[0].weight.data[:, :orig_first_linear.in_features] = orig_first_linear.weight.data.clone()\n",
        "\n",
        "    new_classifier[0].bias.data = orig_first_linear.bias.data.clone()\n",
        "\n",
        "    for i in [3, 6]:\n",
        "        new_classifier[i].weight.data = original_model.classifier[i].weight.data.clone()\n",
        "        new_classifier[i].bias.data = original_model.classifier[i].bias.data.clone()\n",
        "\n",
        "    class PrunedVGG(nn.Module):\n",
        "        def __init__(self, features, classifier):\n",
        "            super().__init__()\n",
        "            self.features = features\n",
        "            self.classifier = classifier\n",
        "\n",
        "        def forward(self, x):\n",
        "            out = self.features(x)\n",
        "            out = out.view(out.size(0), -1)\n",
        "            out = self.classifier(out)\n",
        "            return out\n",
        "\n",
        "    pruned_model = PrunedVGG(nn.Sequential(*new_features), new_classifier)\n",
        "    return pruned_model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o9Aq3AI87Oo8"
      },
      "source": [
        " # Train and test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oG2NWcwqk2O8"
      },
      "outputs": [],
      "source": [
        "def train(model, loader, optimizer, criterion,device, mask_infos=None, weight_mask_infos=None):\n",
        "    model.train()\n",
        "    total_loss, total_correct, total = 0.0, 0, 0\n",
        "\n",
        "    for x, y in loader:\n",
        "        x, y = x.to(device), y.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        logits = model(x)\n",
        "        loss = criterion(logits, y)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "        _, predicted = logits.max(1)\n",
        "        total += y.size(0)\n",
        "        total_correct += predicted.eq(y).sum().item()\n",
        "\n",
        "    train_loss = total_loss / len(trainloader)\n",
        "    train_acc = 100. * total_correct / total\n",
        "\n",
        "    return train_loss, train_acc\n",
        "\n",
        "@torch.no_grad()\n",
        "def evaluate(model, loader, criterion, device):\n",
        "    model.eval()\n",
        "    total_loss, total_correct, total = 0.0, 0, 0\n",
        "\n",
        "    for x, y in loader:\n",
        "        x, y = x.to(device), y.to(device)\n",
        "        logits = model(x)\n",
        "        loss = criterion(logits, y)\n",
        "\n",
        "        total_loss += loss.item()\n",
        "        _, predicted = logits.max(1)\n",
        "        total += y.size(0)\n",
        "        total_correct += predicted.eq(y).sum().item()\n",
        "\n",
        "    total_loss = total_loss / len(testloader)\n",
        "    test_acc = 100. * total_correct / total\n",
        "\n",
        "    return total_loss, test_acc\n",
        "\n",
        "def count_parameters(model):\n",
        "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "def count_nonzero_parameters(model):\n",
        "    return sum((p != 0).sum().item() for p in model.parameters() if p.requires_grad)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "saFiqbgcjYYP"
      },
      "source": [
        "# Hyperparameter"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EQIr2cJJjBD1"
      },
      "outputs": [],
      "source": [
        "LEARNING_RATE = 0.01\n",
        "MOMENTUM = 0.9\n",
        "WEIGHT_DECAY = 5e-4\n",
        "EPOCHS = 100\n",
        "\n",
        "SPARSITY = 0.9\n",
        "GSRP_WEIGHTWISE_SPARSITY = 0.9\n",
        "GSRP_STRUCTURED_SPARSITY =  0.3\n",
        "GSRP_STRUCTURED_ONLY = 0.5\n",
        "GSRP_HYBRID_WEIGHTWISE = 0.8\n",
        "\n",
        "T_SCORE = 3\n",
        "CHECKPOINT_DIR = './checkpoints'\n",
        "os.makedirs(CHECKPOINT_DIR, exist_ok=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UD-METtejx-b"
      },
      "outputs": [],
      "source": [
        "results = {\n",
        "    'original':        {'train_acc': [], 'test_acc': [], 'train_loss': [], 'test_loss': [], 'epoch_time': []},\n",
        "    'GSRP_weightwise': {'train_acc': [], 'test_acc': [], 'train_loss': [], 'test_loss': [], 'epoch_time': []},\n",
        "    'GSRP_hybrid':     {'train_acc': [], 'test_acc': [], 'train_loss': [], 'test_loss': [], 'epoch_time': []},\n",
        "    'GSRP_structure':  {'train_acc': [], 'test_acc': [], 'train_loss': [], 'test_loss': [], 'epoch_time': []},\n",
        "    'snip':            {'train_acc': [], 'test_acc': [], 'train_loss': [], 'test_loss': [], 'epoch_time': []},\n",
        "    'random':          {'train_acc': [], 'test_acc': [], 'train_loss': [], 'test_loss': [], 'epoch_time': []},\n",
        "}\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dSq__ajFDko8"
      },
      "source": [
        "# Method1: original"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tEsm7s4u9_SB"
      },
      "outputs": [],
      "source": [
        "print(\"=\" * 70)\n",
        "print(\"Training original model (no pruning)\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "# Check if checkpoint exists\n",
        "checkpoint_path = os.path.join(CHECKPOINT_DIR, 'model_original.pth')\n",
        "start_epoch = 0\n",
        "\n",
        "model_original = VGG('VGG16').to(device)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer_original = optim.SGD(model_original.parameters(), lr=LEARNING_RATE,\n",
        "                               momentum=MOMENTUM, weight_decay=WEIGHT_DECAY)\n",
        "scheduler_original = optim.lr_scheduler.MultiStepLR(optimizer_original,\n",
        "                                                    milestones=[25, 40], gamma=0.1)\n",
        "\n",
        "# Try to load checkpoint\n",
        "if os.path.exists(checkpoint_path):\n",
        "    print(f\"Checkpoint found, load it? (y/n)\")\n",
        "    # Auto-load in Colab\n",
        "    load_existing = True  # Set to False to retrain\n",
        "\n",
        "    if load_existing:\n",
        "        model_original, optimizer_original, start_epoch, loaded_results, _ = \\\n",
        "            load_checkpoint(model_original, optimizer_original, checkpoint_path)\n",
        "        if loaded_results is not None:\n",
        "            results['original'] = loaded_results\n",
        "            print(f\"Continue training from Epoch {start_epoch}\")\n",
        "\n",
        "total_params = count_parameters(model_original)\n",
        "print(f\"# of parameter: {total_params:,}\")\n",
        "\n",
        "if start_epoch < EPOCHS:\n",
        "    start_time = time.time()\n",
        "\n",
        "    for epoch in range(start_epoch, EPOCHS):\n",
        "        e_start_time = time.time()\n",
        "        train_loss, train_acc = train(model_original, trainloader,\n",
        "                                           optimizer_original, criterion, device)\n",
        "        test_loss, test_acc = evaluate(model_original, testloader, criterion, device)\n",
        "        e_end_time = time.time()\n",
        "        epoch_time = e_end_time - e_start_time\n",
        "\n",
        "        results['original']['train_acc'].append(train_acc)\n",
        "        results['original']['test_acc'].append(test_acc)\n",
        "        results['original']['train_loss'].append(train_loss)\n",
        "        results['original']['test_loss'].append(test_loss)\n",
        "        results['original']['epoch_time'].append(epoch_time)\n",
        "\n",
        "        scheduler_original.step()\n",
        "\n",
        "        print(f\"Epoch [{epoch+1}/{EPOCHS}] \"\n",
        "              f\"Train Loss: {train_loss:.3f}, Train Acc: {train_acc:.2f}%, \"\n",
        "              f\"Test Loss: {test_loss:.3f}, Test Acc: {test_acc:.2f}%\")\n",
        "\n",
        "        # Save checkpoint every 10 epochs\n",
        "        if (epoch + 1) % 10 == 0:\n",
        "            save_checkpoint(model_original, results['original'], optimizer_original,\n",
        "                          epoch + 1, checkpoint_path, 'original')\n",
        "\n",
        "    original_time = time.time() - start_time\n",
        "\n",
        "    # Training complete, save final checkpoint\n",
        "    save_checkpoint(model_original, results['original'], optimizer_original,\n",
        "                   EPOCHS, checkpoint_path, 'original')\n",
        "\n",
        "    print(f\"Training time: {original_time/60:.2f} minutes\")\n",
        "else:\n",
        "    print(\"Model training already completed\")\n",
        "\n",
        "print(f\"Accuracy: {results['original']['test_acc'][-1]:.2f}%\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_4O2wBYuECXI"
      },
      "source": [
        "# Method 2: GSRP weight-wise (unstructured)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "1vB8ZylMEBk1",
        "outputId": "8e64e512-b706-4519-cc1c-88203263d4be"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "================================================================================\n",
            "METHOD 2: GSRP WEIGHT-WISE (UNSTRUCTURED)\n",
            "================================================================================\n",
            "找不到 checkpoint: ./checkpoints/model_GSRP_weightwise.pth\n",
            "剪枝遮罩已儲存至 ./checkpoints/masks_GSRP_weightwise.pth\n",
            "Epoch [1/100] Train Loss: 1.725, Train Acc: 32.59%, Test Loss: 1.441, Test Acc: 47.55%\n",
            "Epoch [2/100] Train Loss: 1.181, Train Acc: 57.84%, Test Loss: 1.119, Test Acc: 62.46%\n",
            "Epoch [3/100] Train Loss: 0.924, Train Acc: 68.50%, Test Loss: 0.930, Test Acc: 69.07%\n",
            "Epoch [4/100] Train Loss: 0.799, Train Acc: 73.47%, Test Loss: 0.788, Test Acc: 73.51%\n",
            "Epoch [5/100] Train Loss: 0.715, Train Acc: 76.38%, Test Loss: 0.725, Test Acc: 76.83%\n",
            "Epoch [6/100] Train Loss: 0.645, Train Acc: 79.03%, Test Loss: 0.783, Test Acc: 74.21%\n",
            "Epoch [7/100] Train Loss: 0.587, Train Acc: 81.04%, Test Loss: 0.785, Test Acc: 75.16%\n",
            "Epoch [8/100] Train Loss: 0.547, Train Acc: 82.31%, Test Loss: 0.685, Test Acc: 77.84%\n",
            "Epoch [9/100] Train Loss: 0.506, Train Acc: 83.57%, Test Loss: 0.547, Test Acc: 82.34%\n",
            "Epoch [10/100] Train Loss: 0.481, Train Acc: 84.52%, Test Loss: 0.466, Test Acc: 84.35%\n",
            "Checkpoint 已儲存至 ./checkpoints/model_GSRP_weightwise.pth\n",
            "Epoch [11/100] Train Loss: 0.455, Train Acc: 85.39%, Test Loss: 0.567, Test Acc: 82.01%\n",
            "Epoch [12/100] Train Loss: 0.434, Train Acc: 86.23%, Test Loss: 0.552, Test Acc: 82.43%\n",
            "Epoch [13/100] Train Loss: 0.407, Train Acc: 86.89%, Test Loss: 0.470, Test Acc: 84.79%\n",
            "Epoch [14/100] Train Loss: 0.386, Train Acc: 87.49%, Test Loss: 0.510, Test Acc: 83.59%\n",
            "Epoch [15/100] Train Loss: 0.372, Train Acc: 87.90%, Test Loss: 0.539, Test Acc: 83.06%\n",
            "Epoch [16/100] Train Loss: 0.355, Train Acc: 88.63%, Test Loss: 0.391, Test Acc: 87.03%\n",
            "Epoch [17/100] Train Loss: 0.342, Train Acc: 89.02%, Test Loss: 0.396, Test Acc: 86.90%\n",
            "Epoch [18/100] Train Loss: 0.331, Train Acc: 89.18%, Test Loss: 0.416, Test Acc: 86.17%\n",
            "Epoch [19/100] Train Loss: 0.316, Train Acc: 90.02%, Test Loss: 0.393, Test Acc: 87.37%\n",
            "Epoch [20/100] Train Loss: 0.308, Train Acc: 90.00%, Test Loss: 0.478, Test Acc: 84.95%\n",
            "Checkpoint 已儲存至 ./checkpoints/model_GSRP_weightwise.pth\n",
            "Epoch [21/100] Train Loss: 0.302, Train Acc: 90.10%, Test Loss: 0.457, Test Acc: 85.06%\n",
            "Epoch [22/100] Train Loss: 0.291, Train Acc: 90.55%, Test Loss: 0.411, Test Acc: 87.04%\n",
            "Epoch [23/100] Train Loss: 0.280, Train Acc: 90.96%, Test Loss: 0.404, Test Acc: 86.97%\n",
            "Epoch [24/100] Train Loss: 0.269, Train Acc: 91.32%, Test Loss: 0.422, Test Acc: 86.63%\n",
            "Epoch [25/100] Train Loss: 0.262, Train Acc: 91.56%, Test Loss: 0.425, Test Acc: 87.18%\n",
            "Epoch [26/100] Train Loss: 0.181, Train Acc: 94.17%, Test Loss: 0.303, Test Acc: 90.47%\n",
            "Epoch [27/100] Train Loss: 0.157, Train Acc: 94.94%, Test Loss: 0.300, Test Acc: 90.64%\n",
            "Epoch [28/100] Train Loss: 0.142, Train Acc: 95.43%, Test Loss: 0.300, Test Acc: 90.98%\n",
            "Epoch [29/100] Train Loss: 0.133, Train Acc: 95.79%, Test Loss: 0.305, Test Acc: 90.68%\n",
            "Epoch [30/100] Train Loss: 0.130, Train Acc: 95.89%, Test Loss: 0.304, Test Acc: 90.92%\n",
            "Checkpoint 已儲存至 ./checkpoints/model_GSRP_weightwise.pth\n",
            "Epoch [31/100] Train Loss: 0.123, Train Acc: 96.07%, Test Loss: 0.305, Test Acc: 91.08%\n",
            "Epoch [32/100] Train Loss: 0.119, Train Acc: 96.21%, Test Loss: 0.309, Test Acc: 90.90%\n",
            "Epoch [33/100] Train Loss: 0.114, Train Acc: 96.32%, Test Loss: 0.306, Test Acc: 90.86%\n",
            "Epoch [34/100] Train Loss: 0.110, Train Acc: 96.49%, Test Loss: 0.310, Test Acc: 91.08%\n",
            "Epoch [35/100] Train Loss: 0.108, Train Acc: 96.54%, Test Loss: 0.314, Test Acc: 91.04%\n",
            "Epoch [36/100] Train Loss: 0.108, Train Acc: 96.57%, Test Loss: 0.313, Test Acc: 91.22%\n",
            "Epoch [37/100] Train Loss: 0.102, Train Acc: 96.69%, Test Loss: 0.321, Test Acc: 91.10%\n",
            "Epoch [38/100] Train Loss: 0.095, Train Acc: 96.89%, Test Loss: 0.318, Test Acc: 91.22%\n",
            "Epoch [39/100] Train Loss: 0.097, Train Acc: 96.89%, Test Loss: 0.322, Test Acc: 91.21%\n",
            "Epoch [40/100] Train Loss: 0.094, Train Acc: 96.95%, Test Loss: 0.332, Test Acc: 90.92%\n",
            "Checkpoint 已儲存至 ./checkpoints/model_GSRP_weightwise.pth\n",
            "Epoch [41/100] Train Loss: 0.083, Train Acc: 97.40%, Test Loss: 0.324, Test Acc: 91.24%\n",
            "Epoch [42/100] Train Loss: 0.082, Train Acc: 97.39%, Test Loss: 0.319, Test Acc: 91.28%\n",
            "Epoch [43/100] Train Loss: 0.081, Train Acc: 97.35%, Test Loss: 0.321, Test Acc: 91.26%\n",
            "Epoch [44/100] Train Loss: 0.080, Train Acc: 97.44%, Test Loss: 0.318, Test Acc: 91.33%\n",
            "Epoch [45/100] Train Loss: 0.078, Train Acc: 97.45%, Test Loss: 0.321, Test Acc: 91.30%\n",
            "Epoch [46/100] Train Loss: 0.078, Train Acc: 97.54%, Test Loss: 0.320, Test Acc: 91.25%\n",
            "Epoch [47/100] Train Loss: 0.079, Train Acc: 97.45%, Test Loss: 0.320, Test Acc: 91.23%\n",
            "Epoch [48/100] Train Loss: 0.076, Train Acc: 97.65%, Test Loss: 0.320, Test Acc: 91.21%\n",
            "Epoch [49/100] Train Loss: 0.079, Train Acc: 97.56%, Test Loss: 0.321, Test Acc: 91.26%\n",
            "Epoch [50/100] Train Loss: 0.077, Train Acc: 97.52%, Test Loss: 0.322, Test Acc: 91.24%\n",
            "Checkpoint 已儲存至 ./checkpoints/model_GSRP_weightwise.pth\n",
            "Epoch [51/100] Train Loss: 0.074, Train Acc: 97.61%, Test Loss: 0.320, Test Acc: 91.30%\n",
            "Epoch [52/100] Train Loss: 0.075, Train Acc: 97.66%, Test Loss: 0.326, Test Acc: 91.25%\n",
            "Epoch [53/100] Train Loss: 0.076, Train Acc: 97.64%, Test Loss: 0.323, Test Acc: 91.37%\n",
            "Epoch [54/100] Train Loss: 0.078, Train Acc: 97.55%, Test Loss: 0.324, Test Acc: 91.29%\n",
            "Epoch [55/100] Train Loss: 0.075, Train Acc: 97.65%, Test Loss: 0.328, Test Acc: 91.28%\n",
            "Epoch [56/100] Train Loss: 0.071, Train Acc: 97.75%, Test Loss: 0.325, Test Acc: 91.28%\n",
            "Epoch [57/100] Train Loss: 0.075, Train Acc: 97.59%, Test Loss: 0.330, Test Acc: 91.24%\n",
            "Epoch [58/100] Train Loss: 0.073, Train Acc: 97.63%, Test Loss: 0.327, Test Acc: 91.33%\n",
            "Epoch [59/100] Train Loss: 0.073, Train Acc: 97.72%, Test Loss: 0.328, Test Acc: 91.32%\n",
            "Epoch [60/100] Train Loss: 0.073, Train Acc: 97.70%, Test Loss: 0.329, Test Acc: 91.29%\n",
            "Checkpoint 已儲存至 ./checkpoints/model_GSRP_weightwise.pth\n",
            "Epoch [61/100] Train Loss: 0.072, Train Acc: 97.73%, Test Loss: 0.329, Test Acc: 91.24%\n",
            "Epoch [62/100] Train Loss: 0.071, Train Acc: 97.72%, Test Loss: 0.331, Test Acc: 91.13%\n",
            "Epoch [63/100] Train Loss: 0.069, Train Acc: 97.78%, Test Loss: 0.332, Test Acc: 91.20%\n",
            "Epoch [64/100] Train Loss: 0.070, Train Acc: 97.77%, Test Loss: 0.333, Test Acc: 91.18%\n",
            "Epoch [65/100] Train Loss: 0.072, Train Acc: 97.66%, Test Loss: 0.333, Test Acc: 91.15%\n",
            "Epoch [66/100] Train Loss: 0.068, Train Acc: 97.80%, Test Loss: 0.333, Test Acc: 91.14%\n",
            "Epoch [67/100] Train Loss: 0.070, Train Acc: 97.81%, Test Loss: 0.334, Test Acc: 91.22%\n",
            "Epoch [68/100] Train Loss: 0.070, Train Acc: 97.81%, Test Loss: 0.335, Test Acc: 91.15%\n",
            "Epoch [69/100] Train Loss: 0.068, Train Acc: 97.82%, Test Loss: 0.333, Test Acc: 91.18%\n",
            "Epoch [70/100] Train Loss: 0.069, Train Acc: 97.83%, Test Loss: 0.332, Test Acc: 91.30%\n",
            "Checkpoint 已儲存至 ./checkpoints/model_GSRP_weightwise.pth\n",
            "Epoch [71/100] Train Loss: 0.065, Train Acc: 97.93%, Test Loss: 0.332, Test Acc: 91.21%\n",
            "Epoch [72/100] Train Loss: 0.069, Train Acc: 97.86%, Test Loss: 0.335, Test Acc: 91.14%\n",
            "Epoch [73/100] Train Loss: 0.070, Train Acc: 97.74%, Test Loss: 0.335, Test Acc: 91.25%\n",
            "Epoch [74/100] Train Loss: 0.066, Train Acc: 97.88%, Test Loss: 0.335, Test Acc: 91.21%\n",
            "Epoch [75/100] Train Loss: 0.065, Train Acc: 97.85%, Test Loss: 0.335, Test Acc: 91.35%\n",
            "Epoch [76/100] Train Loss: 0.065, Train Acc: 97.92%, Test Loss: 0.332, Test Acc: 91.23%\n",
            "Epoch [77/100] Train Loss: 0.064, Train Acc: 98.01%, Test Loss: 0.338, Test Acc: 91.22%\n",
            "Epoch [78/100] Train Loss: 0.067, Train Acc: 97.90%, Test Loss: 0.339, Test Acc: 91.22%\n",
            "Epoch [79/100] Train Loss: 0.067, Train Acc: 97.92%, Test Loss: 0.338, Test Acc: 91.11%\n",
            "Epoch [80/100] Train Loss: 0.063, Train Acc: 97.97%, Test Loss: 0.335, Test Acc: 91.24%\n",
            "Checkpoint 已儲存至 ./checkpoints/model_GSRP_weightwise.pth\n",
            "Epoch [81/100] Train Loss: 0.064, Train Acc: 97.97%, Test Loss: 0.337, Test Acc: 91.23%\n",
            "Epoch [82/100] Train Loss: 0.066, Train Acc: 97.82%, Test Loss: 0.340, Test Acc: 91.22%\n",
            "Epoch [83/100] Train Loss: 0.064, Train Acc: 98.01%, Test Loss: 0.340, Test Acc: 91.30%\n",
            "Epoch [84/100] Train Loss: 0.061, Train Acc: 98.03%, Test Loss: 0.339, Test Acc: 91.21%\n",
            "Epoch [85/100] Train Loss: 0.063, Train Acc: 97.97%, Test Loss: 0.342, Test Acc: 91.17%\n",
            "Epoch [86/100] Train Loss: 0.063, Train Acc: 97.94%, Test Loss: 0.342, Test Acc: 91.10%\n",
            "Epoch [87/100] Train Loss: 0.061, Train Acc: 98.03%, Test Loss: 0.342, Test Acc: 91.24%\n",
            "Epoch [88/100] Train Loss: 0.063, Train Acc: 98.03%, Test Loss: 0.343, Test Acc: 91.18%\n",
            "Epoch [89/100] Train Loss: 0.061, Train Acc: 98.06%, Test Loss: 0.342, Test Acc: 91.24%\n",
            "Epoch [90/100] Train Loss: 0.059, Train Acc: 98.12%, Test Loss: 0.342, Test Acc: 91.24%\n",
            "Checkpoint 已儲存至 ./checkpoints/model_GSRP_weightwise.pth\n",
            "Epoch [91/100] Train Loss: 0.061, Train Acc: 98.04%, Test Loss: 0.342, Test Acc: 91.26%\n",
            "Epoch [92/100] Train Loss: 0.061, Train Acc: 98.05%, Test Loss: 0.342, Test Acc: 91.28%\n",
            "Epoch [93/100] Train Loss: 0.061, Train Acc: 98.03%, Test Loss: 0.342, Test Acc: 91.20%\n",
            "Epoch [94/100] Train Loss: 0.060, Train Acc: 98.05%, Test Loss: 0.341, Test Acc: 91.31%\n",
            "Epoch [95/100] Train Loss: 0.059, Train Acc: 98.13%, Test Loss: 0.344, Test Acc: 91.30%\n",
            "Epoch [96/100] Train Loss: 0.061, Train Acc: 98.05%, Test Loss: 0.346, Test Acc: 91.05%\n",
            "Epoch [97/100] Train Loss: 0.058, Train Acc: 98.14%, Test Loss: 0.342, Test Acc: 91.26%\n",
            "Epoch [98/100] Train Loss: 0.059, Train Acc: 98.07%, Test Loss: 0.345, Test Acc: 91.24%\n",
            "Epoch [99/100] Train Loss: 0.059, Train Acc: 98.11%, Test Loss: 0.345, Test Acc: 91.36%\n",
            "Epoch [100/100] Train Loss: 0.062, Train Acc: 98.11%, Test Loss: 0.346, Test Acc: 91.19%\n",
            "Checkpoint 已儲存至 ./checkpoints/model_GSRP_weightwise.pth\n",
            "Checkpoint 已儲存至 ./checkpoints/model_GSRP_weightwise.pth\n",
            "Training time: 43.62 minutes\n",
            "Accuracy: 91.19%\n"
          ]
        }
      ],
      "source": [
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"METHOD 2: GSRP WEIGHT-WISE (UNSTRUCTURED)\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "checkpoint_path_GSRP_weightwise = os.path.join(CHECKPOINT_DIR, 'model_GSRP_weightwise.pth')\n",
        "masks_path_GSRP_weightwise = os.path.join(CHECKPOINT_DIR, 'masks_GSRP_weightwise.pth')\n",
        "start_epoch = 0\n",
        "\n",
        "model_GSRP_weightwise = VGG('VGG16').to(device)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "\n",
        "# Try to load checkpoint\n",
        "if os.path.exists(masks_path_GSRP_weightwise):\n",
        "    load_existing = True  # Set to False to retrain\n",
        "\n",
        "    if load_existing:\n",
        "        optimizer_GSRP_weightwise = optim.SGD(model_GSRP_weightwise.parameters(), lr=LEARNING_RATE,\n",
        "                                   momentum=MOMENTUM, weight_decay=WEIGHT_DECAY)\n",
        "        model_GSRP_weightwise, optimizer_GSRP_weightwise, start_epoch, loaded_results, _ = \\\n",
        "            load_checkpoint(model_GSRP_weightwise, optimizer_GSRP_weightwise, checkpoint_path_GSRP_weightwise)\n",
        "        if loaded_results is not None:\n",
        "            results['GSRP_weightwise'] = loaded_results\n",
        "            # Load pruning masks\n",
        "            keep_masks = load_pruning_masks(masks_path_GSRP_weightwise)\n",
        "            if keep_masks is not None:\n",
        "                apply_prune_mask(model_GSRP_weightwise, keep_masks)\n",
        "            print(f\"Continue training from Epoch {start_epoch}\")\n",
        "\n",
        "if start_epoch == 0:\n",
        "    # Perform GSRP_weightwise pruning\n",
        "    param_infos = compute_gsrp_scores_weightwise(model_GSRP_weightwise, score_loader, device, T=T_SCORE)\n",
        "    param_infos = choose_weights_by_gsrp_weightwise(param_infos, target_sparsity=GSRP_HYBRID_WEIGHTWISE)\n",
        "    keep_masks = [info[\"mask\"].to(device).float() for info in param_infos]\n",
        "\n",
        "    apply_prune_mask(model_GSRP_weightwise, keep_masks)\n",
        "\n",
        "    # Save pruning masks\n",
        "    save_pruning_masks(keep_masks, masks_path_GSRP_weightwise)\n",
        "\n",
        "if start_epoch < EPOCHS:\n",
        "    optimizer_GSRP_weightwise = optim.SGD(model_GSRP_weightwise.parameters(), lr=LEARNING_RATE,\n",
        "                               momentum=MOMENTUM, weight_decay=WEIGHT_DECAY)\n",
        "    scheduler_GSRP_weightwise = optim.lr_scheduler.MultiStepLR(optimizer_GSRP_weightwise,\n",
        "                                                   milestones=[25, 40], gamma=0.1)\n",
        "\n",
        "    start_time = time.time()\n",
        "\n",
        "    for epoch in range(start_epoch, EPOCHS):\n",
        "        e_start_time = time.time()\n",
        "        train_loss, train_acc = train(model_GSRP_weightwise, trainloader,\n",
        "                                           optimizer_GSRP_weightwise, criterion, device)\n",
        "        test_loss, test_acc = evaluate(model_GSRP_weightwise, testloader, criterion, device)\n",
        "\n",
        "        e_end_time = time.time()\n",
        "        epoch_time = e_end_time - e_start_time\n",
        "\n",
        "        results['GSRP_weightwise']['train_acc'].append(train_acc)\n",
        "        results['GSRP_weightwise']['test_acc'].append(test_acc)\n",
        "        results['GSRP_weightwise']['train_loss'].append(train_loss)\n",
        "        results['GSRP_weightwise']['test_loss'].append(test_loss)\n",
        "        results['GSRP_weightwise']['epoch_time'].append(epoch_time)\n",
        "\n",
        "        scheduler_GSRP_weightwise.step()\n",
        "\n",
        "        print(f\"Epoch [{epoch+1}/{EPOCHS}] \"\n",
        "              f\"Train Loss: {train_loss:.3f}, Train Acc: {train_acc:.2f}%, \"\n",
        "              f\"Test Loss: {test_loss:.3f}, Test Acc: {test_acc:.2f}%\")\n",
        "\n",
        "        # Save checkpoint every 10 epochs\n",
        "        if (epoch + 1) % 10 == 0:\n",
        "            save_checkpoint(model_GSRP_weightwise, results['GSRP_weightwise'], optimizer_GSRP_weightwise,\n",
        "                          epoch + 1, checkpoint_path_GSRP_weightwise, 'GSRP_weightwise')\n",
        "\n",
        "    GSRP_weightwise_time = time.time() - start_time\n",
        "\n",
        "    # Training complete, save final checkpoint\n",
        "    save_checkpoint(model_GSRP_weightwise, results['GSRP_weightwise'], optimizer_GSRP_weightwise,\n",
        "                   EPOCHS, checkpoint_path_GSRP_weightwise, 'GSRP_weightwise')\n",
        "\n",
        "    print(f\"Training time: {GSRP_weightwise_time/60:.2f} minutes\")\n",
        "else:\n",
        "    print(\"Model training already completed\")\n",
        "\n",
        "print(f\"Accuracy: {results['GSRP_weightwise']['test_acc'][-1]:.2f}%\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4teA9jeFEOFl"
      },
      "source": [
        "# Method 3: GSRP Hybrid (Structured + Weight-wise)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "vIB4gb98Dyq7",
        "outputId": "64d120e4-5a77-4d89-bef8-d86dc9be91bf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "================================================================================\n",
            "METHOD 3: GSRP Hybrid (Structured + Weight-wise) Pruning\n",
            "================================================================================\n",
            "After structured pruning:\n",
            "  Original: 15,253,578 params\n",
            "  Pruned:   7,649,827 params\n",
            "\n",
            "After weight-wise pruning on top:\n",
            "  Total parameters: 7,649,827\n",
            "  Zero parameters: 6,114,904\n",
            "  Weight sparsity: 79.94%\n",
            "  Combined reduction: Physical pruning + 80% zeros\n",
            "Epoch [1/50] Train Loss: 1.783, Train Acc: 30.08%, Test Loss: 1.499, Test Acc: 42.15%\n",
            "Epoch [2/50] Train Loss: 1.321, Train Acc: 52.27%, Test Loss: 1.237, Test Acc: 58.25%\n",
            "Epoch [3/50] Train Loss: 1.037, Train Acc: 64.26%, Test Loss: 0.901, Test Acc: 68.92%\n",
            "Epoch [4/50] Train Loss: 0.891, Train Acc: 69.82%, Test Loss: 0.839, Test Acc: 71.15%\n",
            "Epoch [5/50] Train Loss: 0.784, Train Acc: 73.79%, Test Loss: 0.721, Test Acc: 75.35%\n",
            "Epoch [6/50] Train Loss: 0.714, Train Acc: 76.64%, Test Loss: 0.759, Test Acc: 74.75%\n",
            "Epoch [7/50] Train Loss: 0.659, Train Acc: 78.50%, Test Loss: 0.663, Test Acc: 78.15%\n",
            "Epoch [8/50] Train Loss: 0.610, Train Acc: 80.19%, Test Loss: 0.664, Test Acc: 78.12%\n",
            "Epoch [9/50] Train Loss: 0.573, Train Acc: 81.38%, Test Loss: 0.659, Test Acc: 78.78%\n",
            "Epoch [10/50] Train Loss: 0.535, Train Acc: 82.69%, Test Loss: 0.585, Test Acc: 81.05%\n",
            "Checkpoint 已儲存至 ./checkpoints/model_GSRP_hybrid.pth\n",
            "Epoch [11/50] Train Loss: 0.514, Train Acc: 83.43%, Test Loss: 0.607, Test Acc: 80.60%\n",
            "Epoch [12/50] Train Loss: 0.487, Train Acc: 84.34%, Test Loss: 0.628, Test Acc: 79.80%\n",
            "Epoch [13/50] Train Loss: 0.473, Train Acc: 84.77%, Test Loss: 0.558, Test Acc: 81.18%\n",
            "Epoch [14/50] Train Loss: 0.451, Train Acc: 85.71%, Test Loss: 0.528, Test Acc: 83.09%\n",
            "Epoch [15/50] Train Loss: 0.437, Train Acc: 85.91%, Test Loss: 0.472, Test Acc: 84.68%\n",
            "Epoch [16/50] Train Loss: 0.417, Train Acc: 86.59%, Test Loss: 0.493, Test Acc: 84.11%\n",
            "Epoch [17/50] Train Loss: 0.410, Train Acc: 87.00%, Test Loss: 0.540, Test Acc: 83.00%\n",
            "Epoch [18/50] Train Loss: 0.384, Train Acc: 87.55%, Test Loss: 0.481, Test Acc: 84.19%\n",
            "Epoch [19/50] Train Loss: 0.378, Train Acc: 87.74%, Test Loss: 0.448, Test Acc: 85.34%\n",
            "Epoch [20/50] Train Loss: 0.370, Train Acc: 88.28%, Test Loss: 0.508, Test Acc: 83.74%\n",
            "Checkpoint 已儲存至 ./checkpoints/model_GSRP_hybrid.pth\n",
            "Epoch [21/50] Train Loss: 0.357, Train Acc: 88.51%, Test Loss: 0.468, Test Acc: 85.04%\n",
            "Epoch [22/50] Train Loss: 0.344, Train Acc: 88.77%, Test Loss: 0.441, Test Acc: 85.78%\n",
            "Epoch [23/50] Train Loss: 0.342, Train Acc: 88.89%, Test Loss: 0.435, Test Acc: 86.07%\n",
            "Epoch [24/50] Train Loss: 0.330, Train Acc: 89.34%, Test Loss: 0.437, Test Acc: 85.66%\n",
            "Epoch [25/50] Train Loss: 0.321, Train Acc: 89.66%, Test Loss: 0.449, Test Acc: 85.72%\n",
            "Epoch [26/50] Train Loss: 0.242, Train Acc: 92.26%, Test Loss: 0.339, Test Acc: 89.08%\n",
            "Epoch [27/50] Train Loss: 0.211, Train Acc: 93.17%, Test Loss: 0.335, Test Acc: 89.38%\n",
            "Epoch [28/50] Train Loss: 0.199, Train Acc: 93.55%, Test Loss: 0.330, Test Acc: 89.74%\n",
            "Epoch [29/50] Train Loss: 0.192, Train Acc: 93.77%, Test Loss: 0.338, Test Acc: 89.46%\n",
            "Epoch [30/50] Train Loss: 0.188, Train Acc: 93.99%, Test Loss: 0.339, Test Acc: 89.54%\n",
            "Checkpoint 已儲存至 ./checkpoints/model_GSRP_hybrid.pth\n",
            "Epoch [31/50] Train Loss: 0.175, Train Acc: 94.25%, Test Loss: 0.342, Test Acc: 89.69%\n",
            "Epoch [32/50] Train Loss: 0.176, Train Acc: 94.31%, Test Loss: 0.338, Test Acc: 89.64%\n",
            "Epoch [33/50] Train Loss: 0.171, Train Acc: 94.38%, Test Loss: 0.339, Test Acc: 89.72%\n",
            "Epoch [34/50] Train Loss: 0.167, Train Acc: 94.54%, Test Loss: 0.345, Test Acc: 89.52%\n",
            "Epoch [35/50] Train Loss: 0.161, Train Acc: 94.73%, Test Loss: 0.352, Test Acc: 89.36%\n",
            "Epoch [36/50] Train Loss: 0.158, Train Acc: 94.87%, Test Loss: 0.343, Test Acc: 89.60%\n",
            "Epoch [37/50] Train Loss: 0.158, Train Acc: 94.85%, Test Loss: 0.348, Test Acc: 89.75%\n",
            "Epoch [38/50] Train Loss: 0.154, Train Acc: 94.98%, Test Loss: 0.351, Test Acc: 89.60%\n",
            "Epoch [39/50] Train Loss: 0.147, Train Acc: 95.27%, Test Loss: 0.349, Test Acc: 89.77%\n",
            "Epoch [40/50] Train Loss: 0.145, Train Acc: 95.33%, Test Loss: 0.355, Test Acc: 89.72%\n",
            "Checkpoint 已儲存至 ./checkpoints/model_GSRP_hybrid.pth\n",
            "Epoch [41/50] Train Loss: 0.139, Train Acc: 95.44%, Test Loss: 0.351, Test Acc: 90.11%\n",
            "Epoch [42/50] Train Loss: 0.132, Train Acc: 95.70%, Test Loss: 0.348, Test Acc: 89.90%\n",
            "Epoch [43/50] Train Loss: 0.132, Train Acc: 95.69%, Test Loss: 0.350, Test Acc: 89.92%\n",
            "Epoch [44/50] Train Loss: 0.136, Train Acc: 95.70%, Test Loss: 0.345, Test Acc: 90.12%\n",
            "Epoch [45/50] Train Loss: 0.132, Train Acc: 95.76%, Test Loss: 0.344, Test Acc: 90.05%\n",
            "Epoch [46/50] Train Loss: 0.130, Train Acc: 95.75%, Test Loss: 0.349, Test Acc: 90.00%\n",
            "Epoch [47/50] Train Loss: 0.129, Train Acc: 95.84%, Test Loss: 0.348, Test Acc: 90.11%\n",
            "Epoch [48/50] Train Loss: 0.130, Train Acc: 95.80%, Test Loss: 0.349, Test Acc: 89.89%\n",
            "Epoch [49/50] Train Loss: 0.130, Train Acc: 95.79%, Test Loss: 0.349, Test Acc: 90.11%\n",
            "Epoch [50/50] Train Loss: 0.128, Train Acc: 95.77%, Test Loss: 0.348, Test Acc: 90.10%\n",
            "Checkpoint 已儲存至 ./checkpoints/model_GSRP_hybrid.pth\n",
            "Checkpoint 已儲存至 ./checkpoints/model_GSRP_hybrid.pth\n",
            "Training time: 22.76 minutes\n",
            "Accuracy: 90.10%\n"
          ]
        }
      ],
      "source": [
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"METHOD 3: GSRP Hybrid (Structured + Weight-wise) Pruning\")\n",
        "print(\"=\" * 80)\n",
        "start_epoch = 0\n",
        "checkpoint_path_GSRP_hybrid = os.path.join(CHECKPOINT_DIR, 'model_GSRP_hybrid.pth')\n",
        "\n",
        "\n",
        "base_model = VGG('VGG16').to(device)\n",
        "\n",
        "# Step 1: Structured pruning with physical removal\n",
        "gsrp_infos3 = compute_gsrp_scores(base_model, score_loader, device, T=T_SCORE)\n",
        "gsrp_infos3 = collect_conv_activations(base_model, score_loader, device, gsrp_infos3, num_batches=5)\n",
        "gsrp_infos3 = choose_filters_by_gsrp(\n",
        "    gsrp_infos3,\n",
        "    target_sparsity=GSRP_STRUCTURED_SPARSITY,\n",
        "    use_clustering=True\n",
        ")\n",
        "\n",
        "model_GSRP_hybrid = prune_vgg_model_physically(base_model, gsrp_infos3).to(device)\n",
        "\n",
        "print(f\"After structured pruning:\")\n",
        "print(f\"  Original: {sum(p.numel() for p in base_model.parameters()):,} params\")\n",
        "print(f\"  Pruned:   {sum(p.numel() for p in model_GSRP_hybrid.parameters()):,} params\")\n",
        "\n",
        "# Step 2: Weight-wise pruning on the physically pruned model\n",
        "param_infos3 = compute_gsrp_scores_weightwise(model_GSRP_hybrid, score_loader, device, T=T_SCORE)\n",
        "param_infos3 = choose_weights_by_gsrp_weightwise(param_infos3, target_sparsity=GSRP_HYBRID_WEIGHTWISE)\n",
        "\n",
        "keep_masks3 = [info[\"mask\"].to(device).float() for info in param_infos3]\n",
        "apply_prune_mask(model_GSRP_hybrid, keep_masks3)\n",
        "\n",
        "\n",
        "total_params = sum(p.numel() for p in model_GSRP_hybrid.parameters())\n",
        "zero_params = sum((p == 0).sum().item() for p in model_GSRP_hybrid.parameters())\n",
        "actual_sparsity = zero_params / total_params\n",
        "print(f\"\\nAfter weight-wise pruning on top:\")\n",
        "print(f\"  Total parameters: {total_params:,}\")\n",
        "print(f\"  Zero parameters: {zero_params:,}\")\n",
        "print(f\"  Weight sparsity: {actual_sparsity:.2%}\")\n",
        "print(f\"  Combined reduction: Physical pruning + {actual_sparsity:.0%} zeros\")\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer_GSRP_hybrid = optim.SGD(\n",
        "    model_GSRP_hybrid.parameters(),\n",
        "    lr=LEARNING_RATE,\n",
        "    momentum=MOMENTUM,\n",
        "    weight_decay=WEIGHT_DECAY\n",
        ")\n",
        "\n",
        "scheduler_GSRP_hybrid = optim.lr_scheduler.MultiStepLR(\n",
        "    optimizer_GSRP_hybrid, milestones=[25, 40], gamma=0.1\n",
        ")\n",
        "\n",
        "start_time = time.time()\n",
        "\n",
        "for epoch in range(start_epoch, EPOCHS):\n",
        "    e_start_time = time.time()\n",
        "    train_loss, train_acc = train(model_GSRP_hybrid, trainloader,\n",
        "                                        optimizer_GSRP_hybrid, criterion, device)\n",
        "    test_loss, test_acc = evaluate(model_GSRP_hybrid, testloader, criterion, device)\n",
        "    e_end_time = time.time()\n",
        "    epoch_time = e_end_time - e_start_time\n",
        "\n",
        "    results['GSRP_hybrid']['train_acc'].append(train_acc)\n",
        "    results['GSRP_hybrid']['test_acc'].append(test_acc)\n",
        "    results['GSRP_hybrid']['train_loss'].append(train_loss)\n",
        "    results['GSRP_hybrid']['test_loss'].append(test_loss)\n",
        "    results['GSRP_hybrid']['epoch_time'].append(epoch_time)\n",
        "\n",
        "\n",
        "    scheduler_GSRP_hybrid.step()\n",
        "\n",
        "    print(f\"Epoch [{epoch+1}/{EPOCHS}] \"\n",
        "          f\"Train Loss: {train_loss:.3f}, Train Acc: {train_acc:.2f}%, \"\n",
        "          f\"Test Loss: {test_loss:.3f}, Test Acc: {test_acc:.2f}%\")\n",
        "\n",
        "    # Save checkpoint every 10 epochs\n",
        "    if (epoch + 1) % 10 == 0:\n",
        "        save_checkpoint(model_GSRP_hybrid, results['GSRP_hybrid'], optimizer_GSRP_hybrid,\n",
        "                      epoch + 1, checkpoint_path_GSRP_hybrid, 'GSRP_hybrid')\n",
        "\n",
        "GSRP_hybrid_time = time.time() - start_time\n",
        "\n",
        "# Training complete, save final checkpoint\n",
        "save_checkpoint(model_GSRP_hybrid, results['GSRP_hybrid'], optimizer_GSRP_hybrid,\n",
        "                EPOCHS, checkpoint_path_GSRP_hybrid, 'GSRP_hybrid')\n",
        "\n",
        "print(f\"Training time: {GSRP_hybrid_time/60:.2f} minutes\")\n",
        "\n",
        "print(f\"Accuracy: {results['GSRP_hybrid']['test_acc'][-1]:.2f}%\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UB06w44wEimd"
      },
      "source": [
        "# Method 4: GSRP structure(gradient+cluster)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "A4v-M24iEi0g",
        "outputId": "d6e077d7-e399-41ed-c854-b108b18fd9d2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "================================================================================\n",
            "METHOD 4: GSRP Structured (Physical Only) Pruning\n",
            "================================================================================\n",
            "Original parameters: 15,253,578\n",
            "Pruned parameters: 4,083,754\n",
            "Parameter reduction: 73.23%\n",
            "Model is now physically smaller and faster!\n",
            "Epoch [1/50] Train Loss: 1.678, Train Acc: 35.22%, Test Loss: 1.289, Test Acc: 53.66%\n",
            "Epoch [2/50] Train Loss: 1.171, Train Acc: 58.69%, Test Loss: 1.049, Test Acc: 63.78%\n",
            "Epoch [3/50] Train Loss: 0.931, Train Acc: 68.10%, Test Loss: 0.860, Test Acc: 70.43%\n",
            "Epoch [4/50] Train Loss: 0.799, Train Acc: 73.00%, Test Loss: 0.760, Test Acc: 75.06%\n",
            "Epoch [5/50] Train Loss: 0.711, Train Acc: 76.36%, Test Loss: 0.699, Test Acc: 76.72%\n",
            "Epoch [6/50] Train Loss: 0.649, Train Acc: 78.42%, Test Loss: 0.666, Test Acc: 78.22%\n",
            "Epoch [7/50] Train Loss: 0.600, Train Acc: 80.32%, Test Loss: 0.586, Test Acc: 80.41%\n",
            "Epoch [8/50] Train Loss: 0.558, Train Acc: 81.72%, Test Loss: 0.558, Test Acc: 81.39%\n",
            "Epoch [9/50] Train Loss: 0.526, Train Acc: 82.63%, Test Loss: 0.509, Test Acc: 83.18%\n",
            "Epoch [10/50] Train Loss: 0.495, Train Acc: 83.76%, Test Loss: 0.650, Test Acc: 78.84%\n",
            "Checkpoint 已儲存至 ./checkpoints/model_GSRP_structure.pth\n",
            "Epoch [11/50] Train Loss: 0.467, Train Acc: 84.68%, Test Loss: 0.517, Test Acc: 83.04%\n",
            "Epoch [12/50] Train Loss: 0.453, Train Acc: 85.13%, Test Loss: 0.487, Test Acc: 83.46%\n",
            "Epoch [13/50] Train Loss: 0.428, Train Acc: 85.98%, Test Loss: 0.551, Test Acc: 82.40%\n",
            "Epoch [14/50] Train Loss: 0.412, Train Acc: 86.37%, Test Loss: 0.484, Test Acc: 84.12%\n",
            "Epoch [15/50] Train Loss: 0.393, Train Acc: 87.08%, Test Loss: 0.535, Test Acc: 82.37%\n",
            "Epoch [16/50] Train Loss: 0.377, Train Acc: 87.44%, Test Loss: 0.474, Test Acc: 84.60%\n",
            "Epoch [17/50] Train Loss: 0.367, Train Acc: 87.96%, Test Loss: 0.473, Test Acc: 84.01%\n",
            "Epoch [18/50] Train Loss: 0.348, Train Acc: 88.45%, Test Loss: 0.441, Test Acc: 85.70%\n",
            "Epoch [19/50] Train Loss: 0.343, Train Acc: 88.60%, Test Loss: 0.417, Test Acc: 86.43%\n",
            "Epoch [20/50] Train Loss: 0.331, Train Acc: 89.21%, Test Loss: 0.443, Test Acc: 85.45%\n",
            "Checkpoint 已儲存至 ./checkpoints/model_GSRP_structure.pth\n",
            "Epoch [21/50] Train Loss: 0.320, Train Acc: 89.51%, Test Loss: 0.447, Test Acc: 85.38%\n",
            "Epoch [22/50] Train Loss: 0.315, Train Acc: 89.58%, Test Loss: 0.386, Test Acc: 87.58%\n",
            "Epoch [23/50] Train Loss: 0.303, Train Acc: 90.04%, Test Loss: 0.435, Test Acc: 86.29%\n",
            "Epoch [24/50] Train Loss: 0.287, Train Acc: 90.50%, Test Loss: 0.415, Test Acc: 86.62%\n",
            "Epoch [25/50] Train Loss: 0.287, Train Acc: 90.54%, Test Loss: 0.394, Test Acc: 87.64%\n",
            "Epoch [26/50] Train Loss: 0.204, Train Acc: 93.22%, Test Loss: 0.321, Test Acc: 89.78%\n",
            "Epoch [27/50] Train Loss: 0.178, Train Acc: 94.23%, Test Loss: 0.321, Test Acc: 89.95%\n",
            "Epoch [28/50] Train Loss: 0.166, Train Acc: 94.55%, Test Loss: 0.327, Test Acc: 90.06%\n",
            "Epoch [29/50] Train Loss: 0.158, Train Acc: 94.81%, Test Loss: 0.325, Test Acc: 89.89%\n",
            "Epoch [30/50] Train Loss: 0.153, Train Acc: 94.87%, Test Loss: 0.325, Test Acc: 90.05%\n",
            "Checkpoint 已儲存至 ./checkpoints/model_GSRP_structure.pth\n",
            "Epoch [31/50] Train Loss: 0.144, Train Acc: 95.24%, Test Loss: 0.327, Test Acc: 90.08%\n",
            "Epoch [32/50] Train Loss: 0.140, Train Acc: 95.45%, Test Loss: 0.331, Test Acc: 90.01%\n",
            "Epoch [33/50] Train Loss: 0.138, Train Acc: 95.40%, Test Loss: 0.330, Test Acc: 90.10%\n",
            "Epoch [34/50] Train Loss: 0.135, Train Acc: 95.51%, Test Loss: 0.335, Test Acc: 90.28%\n",
            "Epoch [35/50] Train Loss: 0.129, Train Acc: 95.69%, Test Loss: 0.332, Test Acc: 90.19%\n",
            "Epoch [36/50] Train Loss: 0.125, Train Acc: 95.87%, Test Loss: 0.336, Test Acc: 90.06%\n",
            "Epoch [37/50] Train Loss: 0.126, Train Acc: 95.87%, Test Loss: 0.342, Test Acc: 89.83%\n",
            "Epoch [38/50] Train Loss: 0.119, Train Acc: 96.15%, Test Loss: 0.341, Test Acc: 89.99%\n",
            "Epoch [39/50] Train Loss: 0.119, Train Acc: 96.04%, Test Loss: 0.343, Test Acc: 90.12%\n",
            "Epoch [40/50] Train Loss: 0.114, Train Acc: 96.27%, Test Loss: 0.341, Test Acc: 90.02%\n",
            "Checkpoint 已儲存至 ./checkpoints/model_GSRP_structure.pth\n",
            "Epoch [41/50] Train Loss: 0.107, Train Acc: 96.46%, Test Loss: 0.339, Test Acc: 90.14%\n",
            "Epoch [42/50] Train Loss: 0.102, Train Acc: 96.63%, Test Loss: 0.336, Test Acc: 90.09%\n",
            "Epoch [43/50] Train Loss: 0.102, Train Acc: 96.61%, Test Loss: 0.340, Test Acc: 90.10%\n",
            "Epoch [44/50] Train Loss: 0.099, Train Acc: 96.80%, Test Loss: 0.338, Test Acc: 90.33%\n",
            "Epoch [45/50] Train Loss: 0.102, Train Acc: 96.65%, Test Loss: 0.340, Test Acc: 90.22%\n",
            "Epoch [46/50] Train Loss: 0.100, Train Acc: 96.71%, Test Loss: 0.340, Test Acc: 90.16%\n",
            "Epoch [47/50] Train Loss: 0.101, Train Acc: 96.67%, Test Loss: 0.341, Test Acc: 90.30%\n",
            "Epoch [48/50] Train Loss: 0.100, Train Acc: 96.77%, Test Loss: 0.339, Test Acc: 90.24%\n",
            "Epoch [49/50] Train Loss: 0.096, Train Acc: 96.81%, Test Loss: 0.341, Test Acc: 90.33%\n",
            "Epoch [50/50] Train Loss: 0.098, Train Acc: 96.79%, Test Loss: 0.338, Test Acc: 90.36%\n",
            "Checkpoint 已儲存至 ./checkpoints/model_GSRP_structure.pth\n",
            "Checkpoint 已儲存至 ./checkpoints/model_GSRP_structure.pth\n",
            "Training time: 43.58 minutes\n",
            "Accuracy: 90.36%\n"
          ]
        }
      ],
      "source": [
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"METHOD 4: GSRP Structured (Physical Only) Pruning\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "\n",
        "checkpoint_path_GSRP_structure = os.path.join(CHECKPOINT_DIR, 'model_GSRP_structure.pth')\n",
        "\n",
        "model_GSRP_structure = VGG('VGG16').to(device)\n",
        "GSRP_structure_temp = VGG('VGG16').to(device)\n",
        "\n",
        "gsrp_infos = compute_gsrp_scores(model_GSRP_structure, score_loader, device, T=T_SCORE)\n",
        "gsrp_infos = collect_conv_activations(model_GSRP_structure, score_loader, device, gsrp_infos, num_batches=5)\n",
        "gsrp_infos = choose_filters_by_gsrp(gsrp_infos, target_sparsity=GSRP_STRUCTURED_ONLY, use_clustering=True)\n",
        "\n",
        "model_GSRP_structure  = prune_vgg_model_physically(model_GSRP_structure, gsrp_infos).to(device)\n",
        "\n",
        "# Count actual parameters\n",
        "original_params = sum(p.numel() for p in GSRP_structure_temp.parameters())\n",
        "pruned_params = sum(p.numel() for p in model_GSRP_structure.parameters())\n",
        "reduction = 100 * (1 - pruned_params / original_params)\n",
        "\n",
        "print(f\"Original parameters: {original_params:,}\")\n",
        "print(f\"Pruned parameters: {pruned_params:,}\")\n",
        "print(f\"Parameter reduction: {reduction:.2f}%\")\n",
        "print(f\"Model is now physically smaller and faster!\")\n",
        "\n",
        "optimizer_GSRP_structure = optim.SGD(model_GSRP_structure.parameters(),\n",
        "                                     lr=LEARNING_RATE,\n",
        "                                     momentum=MOMENTUM,\n",
        "                                     weight_decay=WEIGHT_DECAY)\n",
        "scheduler_GSRP_structure = optim.lr_scheduler.MultiStepLR(\n",
        "    optimizer_GSRP_structure, milestones=[25, 40], gamma=0.1\n",
        ")\n",
        "\n",
        "for epoch in range(start_epoch, EPOCHS):\n",
        "    e_start_time = time.time()\n",
        "    train_loss, train_acc = train(model_GSRP_structure, trainloader,\n",
        "                                        optimizer_GSRP_structure, criterion, device)\n",
        "    test_loss, test_acc = evaluate(model_GSRP_structure, testloader, criterion, device)\n",
        "    e_end_time = time.time()\n",
        "    epoch_time = e_end_time - e_start_time\n",
        "\n",
        "    results['GSRP_structure']['train_acc'].append(train_acc)\n",
        "    results['GSRP_structure']['test_acc'].append(test_acc)\n",
        "    results['GSRP_structure']['train_loss'].append(train_loss)\n",
        "    results['GSRP_structure']['test_loss'].append(test_loss)\n",
        "    results['GSRP_structure']['epoch_time'].append(epoch_time)\n",
        "\n",
        "    scheduler_GSRP_structure.step()\n",
        "\n",
        "    print(f\"Epoch [{epoch+1}/{EPOCHS}] \"\n",
        "          f\"Train Loss: {train_loss:.3f}, Train Acc: {train_acc:.2f}%, \"\n",
        "          f\"Test Loss: {test_loss:.3f}, Test Acc: {test_acc:.2f}%\")\n",
        "\n",
        "    # Save checkpoint every 10 epochs\n",
        "    if (epoch + 1) % 10 == 0:\n",
        "        save_checkpoint(model_GSRP_structure, results['GSRP_structure'], optimizer_GSRP_structure,\n",
        "                      epoch + 1, checkpoint_path_GSRP_structure, 'GSRP_structure')\n",
        "\n",
        "GSRP_structure_time = time.time() - start_time\n",
        "\n",
        "# Training complete, save final checkpoint\n",
        "save_checkpoint(model_GSRP_structure, results['GSRP_structure'], optimizer_GSRP_structure,\n",
        "                EPOCHS, checkpoint_path_GSRP_structure, 'GSRP_structure')\n",
        "\n",
        "print(f\"Training time: {GSRP_structure_time/60:.2f} minutes\")\n",
        "\n",
        "print(f\"Accuracy: {results['GSRP_structure']['test_acc'][-1]:.2f}%\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4IPxdqW8etJp"
      },
      "source": [
        "# Method 5: SNIP\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hkq1VO2Od5Iv",
        "outputId": "9bb74861-ba64-49ce-cba9-7f1b7d57a856"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "================================================================================\n",
            "METHOD 5: SNIP Pruning\n",
            "================================================================================\n",
            "SNIP 剪枝完成！剪枝率: 90.0%\n",
            "閾值: 0.000036\n",
            "剪枝遮罩已儲存至 ./checkpoints/masks_snip.pth\n",
            "# of parameter: 1,533,470 (10.05%)\n"
          ]
        }
      ],
      "source": [
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"METHOD 5: SNIP Pruning\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "checkpoint_path_snip = os.path.join(CHECKPOINT_DIR, 'model_snip.pth')\n",
        "masks_path_snip = os.path.join(CHECKPOINT_DIR, 'masks_snip.pth')\n",
        "start_epoch = 0\n",
        "\n",
        "model_snip = VGG('VGG16').to(device)\n",
        "total_params = sum(p.numel() for p in model_snip.parameters())\n",
        "# Try to load checkpoint\n",
        "if os.path.exists(checkpoint_path_snip):\n",
        "    load_existing = True  # Set to False to retrain\n",
        "\n",
        "    if load_existing:\n",
        "        optimizer_snip = optim.SGD(model_snip.parameters(), lr=LEARNING_RATE,\n",
        "                                   momentum=MOMENTUM, weight_decay=WEIGHT_DECAY)\n",
        "        model_snip, optimizer_snip, start_epoch, loaded_results, _ = \\\n",
        "            load_checkpoint(model_snip, optimizer_snip, checkpoint_path_snip)\n",
        "        if loaded_results is not None:\n",
        "            results['snip'] = loaded_results\n",
        "            # Load pruning masks\n",
        "            keep_masks = load_pruning_masks(masks_path_snip)\n",
        "            if keep_masks is not None:\n",
        "                apply_prune_mask(model_snip, keep_masks)\n",
        "            print(f\"Continue training from Epoch {start_epoch}\")\n",
        "\n",
        "if start_epoch == 0:\n",
        "    # Perform SNIP pruning\n",
        "    keep_masks = snip_pruning(model_snip, score_loader, sparsity=SPARSITY)\n",
        "    apply_prune_mask(model_snip, keep_masks)\n",
        "\n",
        "    # Save pruning masks\n",
        "    save_pruning_masks(keep_masks, masks_path_snip)\n",
        "\n",
        "nonzero_params = count_nonzero_parameters(model_snip)\n",
        "print(f\"# of parameter: {nonzero_params:,} ({nonzero_params/total_params*100:.2f}%)\")\n",
        "\n",
        "if start_epoch < EPOCHS:\n",
        "    optimizer_snip = optim.SGD(model_snip.parameters(), lr=LEARNING_RATE,\n",
        "                               momentum=MOMENTUM, weight_decay=WEIGHT_DECAY)\n",
        "    scheduler_snip = optim.lr_scheduler.MultiStepLR(optimizer_snip,\n",
        "                                                   milestones=[25, 40], gamma=0.1)\n",
        "\n",
        "    start_time = time.time()\n",
        "\n",
        "    for epoch in range(start_epoch, EPOCHS):\n",
        "        e_start_time = time.time()\n",
        "        train_loss, train_acc = train(model_snip, trainloader,\n",
        "                                           optimizer_snip, criterion, device)\n",
        "        test_loss, test_acc = evaluate(model_snip, testloader, criterion, device)\n",
        "        e_end_time = time.time()\n",
        "        epoch_time = e_end_time - e_start_time\n",
        "\n",
        "        results['snip']['train_acc'].append(train_acc)\n",
        "        results['snip']['test_acc'].append(test_acc)\n",
        "        results['snip']['train_loss'].append(train_loss)\n",
        "        results['snip']['test_loss'].append(test_loss)\n",
        "        results['snip']['epoch_time'].append(epoch_time)\n",
        "\n",
        "        scheduler_snip.step()\n",
        "\n",
        "        print(f\"Epoch [{epoch+1}/{EPOCHS}] \"\n",
        "              f\"Train Loss: {train_loss:.3f}, Train Acc: {train_acc:.2f}%, \"\n",
        "              f\"Test Loss: {test_loss:.3f}, Test Acc: {test_acc:.2f}%\")\n",
        "\n",
        "        # Save checkpoint every 10 epochs\n",
        "        if (epoch + 1) % 10 == 0:\n",
        "            save_checkpoint(model_snip, results['snip'], optimizer_snip,\n",
        "                          epoch + 1, checkpoint_path_snip, 'snip')\n",
        "\n",
        "    snip_time = time.time() - start_time\n",
        "\n",
        "    # Training complete, save final checkpoint\n",
        "    save_checkpoint(model_snip, results['snip'], optimizer_snip,\n",
        "                   EPOCHS, checkpoint_path_snip, 'snip')\n",
        "\n",
        "    print(f\"Training time: {snip_time/60:.2f} minutes\")\n",
        "else:\n",
        "    print(\"Model training already completed\")\n",
        "\n",
        "print(f\"Accuracy: {results['snip']['test_acc'][-1]:.2f}%\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hJWYFC9pe4Tb"
      },
      "source": [
        "# Random\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sF7O7EEGe8qT"
      },
      "outputs": [],
      "source": [
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"METHOD 6: Random Pruning\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "checkpoint_path_random = os.path.join(CHECKPOINT_DIR, 'model_random.pth')\n",
        "masks_path_random = os.path.join(CHECKPOINT_DIR, 'masks_random.pth')\n",
        "start_epoch = 0\n",
        "\n",
        "model_random = VGG('VGG16').to(device)\n",
        "\n",
        "# Try to load checkpoint\n",
        "if os.path.exists(checkpoint_path_random):\n",
        "    load_existing = True  # Set to False to retrain\n",
        "\n",
        "    if load_existing:\n",
        "        optimizer_random = optim.SGD(model_random.parameters(), lr=LEARNING_RATE,\n",
        "                                     momentum=MOMENTUM, weight_decay=WEIGHT_DECAY)\n",
        "        model_random, optimizer_random, start_epoch, loaded_results, _ = \\\n",
        "            load_checkpoint(model_random, optimizer_random, checkpoint_path_random)\n",
        "        if loaded_results is not None:\n",
        "            results['random'] = loaded_results\n",
        "            # Load pruning masks\n",
        "            keep_masks_random = load_pruning_masks(masks_path_random)\n",
        "            if keep_masks_random is not None:\n",
        "                apply_prune_mask(model_random, keep_masks_random)\n",
        "            print(f\"Continue training from Epoch {start_epoch}\")\n",
        "\n",
        "if start_epoch == 0:\n",
        "    # Perform random pruning\n",
        "    keep_masks_random = random_pruning(model_random, sparsity=SPARSITY)\n",
        "    apply_prune_mask(model_random, keep_masks_random)\n",
        "\n",
        "    # Save pruning masks\n",
        "    save_pruning_masks(keep_masks_random, masks_path_random)\n",
        "\n",
        "nonzero_params_random = count_nonzero_parameters(model_random)\n",
        "print(f\"# of parameter: {nonzero_params_random:,} ({nonzero_params_random/total_params*100:.2f}%)\")\n",
        "\n",
        "if start_epoch < EPOCHS:\n",
        "    optimizer_random = optim.SGD(model_random.parameters(), lr=LEARNING_RATE,\n",
        "                                 momentum=MOMENTUM, weight_decay=WEIGHT_DECAY)\n",
        "    scheduler_random = optim.lr_scheduler.MultiStepLR(optimizer_random,\n",
        "                                                     milestones=[25, 40], gamma=0.1)\n",
        "\n",
        "    start_time = time.time()\n",
        "\n",
        "    for epoch in range(start_epoch, EPOCHS):\n",
        "        e_start_time = time.time()\n",
        "        train_loss, train_acc = train(model_random, trainloader,\n",
        "                                           optimizer_random, criterion, device)\n",
        "        test_loss, test_acc = evaluate(model_random, testloader, criterion, device)\n",
        "        e_end_time = time.time()\n",
        "        epoch_time = e_end_time - e_start_time\n",
        "\n",
        "        results['random']['train_acc'].append(train_acc)\n",
        "        results['random']['test_acc'].append(test_acc)\n",
        "        results['random']['train_loss'].append(train_loss)\n",
        "        results['random']['test_loss'].append(test_loss)\n",
        "        results['random']['epoch_time'].append(epoch_time)\n",
        "\n",
        "        scheduler_random.step()\n",
        "\n",
        "        print(f\"Epoch [{epoch+1}/{EPOCHS}] \"\n",
        "              f\"Train Loss: {train_loss:.3f}, Train Acc: {train_acc:.2f}%, \"\n",
        "              f\"Test Loss: {test_loss:.3f}, Test Acc: {test_acc:.2f}%\")\n",
        "\n",
        "        # Save checkpoint every 10 epochs\n",
        "        if (epoch + 1) % 10 == 0:\n",
        "            save_checkpoint(model_random, results['random'], optimizer_random,\n",
        "                          epoch + 1, checkpoint_path_random, 'random')\n",
        "\n",
        "    random_time = time.time() - start_time\n",
        "\n",
        "    # Training complete, save final checkpoint\n",
        "    save_checkpoint(model_random, results['random'], optimizer_random,\n",
        "                   EPOCHS, checkpoint_path_random, 'random')\n",
        "\n",
        "    print(f\"Training time: {random_time/60:.2f} minutes\")\n",
        "else:\n",
        "    print(\"Model training already completed\")\n",
        "\n",
        "print(f\"Accuracy: {results['random']['test_acc'][-1]:.2f}%\")\n",
        "\n",
        "# Save all results\n",
        "save_results(results, os.path.join(CHECKPOINT_DIR, 'all_results.json'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GEM43OZWqEQB"
      },
      "source": [
        "# Save All Results\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XzqHvoFpqB5f"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "fig, axes = plt.subplots(2, 2, figsize=(16, 10))\n",
        "\n",
        "method_keys   = ['original', 'GSRP_weightwise', 'GSRP_hybrid', 'GSRP_structure', 'snip', 'random']\n",
        "method_labels = ['Original', 'GSRP Weight-wise', 'GSRP Hybrid', 'GSRP Structured', 'SNIP', 'Random']\n",
        "colors        = ['#2E86AB', '#A23B72', '#F18F01', '#06A77D', '#D62839', '#8B4789']\n",
        "\n",
        "# === 1. Training accuracy ===\n",
        "for key, label, c in zip(method_keys, method_labels, colors):\n",
        "    axes[0, 0].plot(results[key]['train_acc'], label=label, linewidth=2.5, alpha=0.8, color=c)\n",
        "axes[0, 0].set_xlabel('Epoch', fontsize=12)\n",
        "axes[0, 0].set_ylabel('Training Accuracy (%)', fontsize=12)\n",
        "axes[0, 0].set_title('Training Accuracy over Epochs', fontsize=14, fontweight='bold')\n",
        "axes[0, 0].legend(fontsize=9)\n",
        "axes[0, 0].grid(True, alpha=0.3)\n",
        "\n",
        "# === 2. Test accuracy ===\n",
        "for key, label, c in zip(method_keys, method_labels, colors):\n",
        "    axes[0, 1].plot(results[key]['test_acc'], label=label, linewidth=2.5, alpha=0.8, color=c)\n",
        "axes[0, 1].set_xlabel('Epoch', fontsize=12)\n",
        "axes[0, 1].set_ylabel('Test Accuracy (%)', fontsize=12)\n",
        "axes[0, 1].set_title('Test Accuracy over Epochs', fontsize=14, fontweight='bold')\n",
        "axes[0, 1].legend(fontsize=9)\n",
        "axes[0, 1].grid(True, alpha=0.3)\n",
        "\n",
        "# === 3. Training loss ===\n",
        "for key, label, c in zip(method_keys, method_labels, colors):\n",
        "    axes[1, 0].plot(results[key]['train_loss'], label=label, linewidth=2.5, alpha=0.8, color=c)\n",
        "axes[1, 0].set_xlabel('Epoch', fontsize=12)\n",
        "axes[1, 0].set_ylabel('Training Loss', fontsize=12)\n",
        "axes[1, 0].set_title('Training Loss over Epochs', fontsize=14, fontweight='bold')\n",
        "axes[1, 0].legend(fontsize=9)\n",
        "axes[1, 0].grid(True, alpha=0.3)\n",
        "\n",
        "# === 4. Final test accuracy bar chart ===\n",
        "final_accs = [results[key]['test_acc'][-1] for key in method_keys]\n",
        "\n",
        "bars = axes[1, 1].bar(method_labels, final_accs, color=colors,\n",
        "                      alpha=0.8, edgecolor='black', linewidth=1.5)\n",
        "axes[1, 1].set_ylabel('Test Accuracy (%)', fontsize=12)\n",
        "axes[1, 1].set_title('Final Test Accuracy Comparison', fontsize=14, fontweight='bold')\n",
        "axes[1, 1].set_ylim([0, 100])\n",
        "axes[1, 1].grid(True, alpha=0.3, axis='y')\n",
        "\n",
        "for bar, acc in zip(bars, final_accs):\n",
        "    height = bar.get_height()\n",
        "    axes[1, 1].text(bar.get_x() + bar.get_width()/2., height + 1,\n",
        "                    f'{acc:.2f}%', ha='center', va='bottom',\n",
        "                    fontsize=9, fontweight='bold')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig('all_methods_comparison.png', dpi=150, bbox_inches='tight')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZFNap9OaZ88r"
      },
      "outputs": [],
      "source": [
        "avg_times = []\n",
        "model_names = []\n",
        "\n",
        "for model_name, stats in results.items():\n",
        "    if stats['epoch_time']:\n",
        "        avg_times.append(sum(stats['epoch_time']) / len(stats['epoch_time']))\n",
        "        model_names.append(model_name)\n",
        "\n",
        "plt.figure()\n",
        "plt.bar(model_names, avg_times)\n",
        "plt.xlabel(\"Model\")\n",
        "plt.ylabel(\"Average time per epoch (seconds)\")\n",
        "plt.title(\"Average Epoch Time per Model\")\n",
        "plt.xticks(rotation=30)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M3nO9YBWOWrk"
      },
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# Load trained models and perform Inference and comparison\n",
        "# No retraining needed, directly use saved checkpoints\n",
        "# ============================================================================\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import json\n",
        "import os\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# ============================================================================\n",
        "# Load model architecture (same as training)\n",
        "# ============================================================================\n",
        "\n",
        "class VGG(nn.Module):\n",
        "    def __init__(self, vgg_name='VGG16', num_classes=10):\n",
        "        super(VGG, self).__init__()\n",
        "        self.cfg = {\n",
        "            'VGG11': [64, 'M', 128, 'M', 256, 256, 'M', 512, 512, 'M', 512, 512, 'M'],\n",
        "            'VGG13': [64, 64, 'M', 128, 128, 'M', 256, 256, 'M', 512, 512, 'M', 512, 512, 'M'],\n",
        "            'VGG16': [64, 64, 'M', 128, 128, 'M', 256, 256, 256, 'M', 512, 512, 512, 'M', 512, 512, 512, 'M'],\n",
        "            'VGG19': [64, 64, 'M', 128, 128, 'M', 256, 256, 256, 256, 'M', 512, 512, 512, 512, 'M', 512, 512, 512, 512, 'M'],\n",
        "        }\n",
        "        self.features = self._make_layers(self.cfg[vgg_name])\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Linear(512, 512),\n",
        "            nn.ReLU(True),\n",
        "            nn.Dropout(),\n",
        "            nn.Linear(512, 512),\n",
        "            nn.ReLU(True),\n",
        "            nn.Dropout(),\n",
        "            nn.Linear(512, num_classes)\n",
        "        )\n",
        "\n",
        "    def _make_layers(self, cfg):\n",
        "        layers = []\n",
        "        in_channels = 3\n",
        "        for x in cfg:\n",
        "            if x == 'M':\n",
        "                layers += [nn.MaxPool2d(kernel_size=2, stride=2)]\n",
        "            else:\n",
        "                layers += [nn.Conv2d(in_channels, x, kernel_size=3, padding=1),\n",
        "                          nn.BatchNorm2d(x),\n",
        "                          nn.ReLU(inplace=True)]\n",
        "                in_channels = x\n",
        "        layers += [nn.AvgPool2d(kernel_size=1, stride=1)]\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.features(x)\n",
        "        out = out.view(out.size(0), -1)\n",
        "        out = self.classifier(out)\n",
        "        return out\n",
        "\n",
        "\n",
        "# ============================================================================\n",
        "# Load test data\n",
        "# ============================================================================\n",
        "\n",
        "transform_test = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
        "])\n",
        "\n",
        "testset = torchvision.datasets.CIFAR10(\n",
        "    root='./data', train=False, download=True, transform=transform_test)\n",
        "testloader = torch.utils.data.DataLoader(\n",
        "    testset, batch_size=100, shuffle=False, num_workers=2)\n",
        "\n",
        "print(f\"Test set size: {len(testset)}\")\n",
        "\n",
        "\n",
        "# ============================================================================\n",
        "# Utility functions\n",
        "# ============================================================================\n",
        "\n",
        "def apply_prune_mask(model, keep_masks):\n",
        "    \"\"\"Apply pruning masks to model weights\"\"\"\n",
        "    prunable_layers = filter(\n",
        "        lambda layer: isinstance(layer, nn.Conv2d) or isinstance(layer, nn.Linear),\n",
        "        model.modules())\n",
        "\n",
        "    for layer, keep_mask in zip(prunable_layers, keep_masks):\n",
        "        assert (layer.weight.shape == keep_mask.shape)\n",
        "\n",
        "        def hook_factory(keep_mask):\n",
        "            def hook(grads):\n",
        "                return grads * keep_mask\n",
        "            return hook\n",
        "\n",
        "        layer.weight.data[keep_mask == 0.] = 0.\n",
        "        layer.weight.register_hook(hook_factory(keep_mask))\n",
        "\n",
        "\n",
        "def test_model(model, testloader, device):\n",
        "    \"\"\"Test model\"\"\"\n",
        "    model.eval()\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    test_loss = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch_idx, (inputs, targets) in enumerate(testloader):\n",
        "            inputs, targets = inputs.to(device), targets.to(device)\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, targets)\n",
        "\n",
        "            test_loss += loss.item()\n",
        "            _, predicted = outputs.max(1)\n",
        "            total += targets.size(0)\n",
        "            correct += predicted.eq(targets).sum().item()\n",
        "\n",
        "    test_loss = test_loss / len(testloader)\n",
        "    test_acc = 100. * correct / total\n",
        "\n",
        "    return test_loss, test_acc\n",
        "\n",
        "\n",
        "def count_nonzero_parameters(model):\n",
        "    \"\"\"Count non-zero parameters\"\"\"\n",
        "    return sum((p != 0).sum().item() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "\n",
        "# ============================================================================\n",
        "# Load all models and perform Inference\n",
        "# ============================================================================\n",
        "\n",
        "CHECKPOINT_DIR = './checkpoints'\n",
        "\n",
        "print(\"\\n\" + \"=\" * 70)\n",
        "print(\"Loading models and performing Inference\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "# Load training results\n",
        "results_path = os.path.join(CHECKPOINT_DIR, 'all_results.json')\n",
        "if os.path.exists(results_path):\n",
        "    with open(results_path, 'r') as f:\n",
        "        results = json.load(f)\n",
        "    print(f\"✓ Training results loaded\")\n",
        "else:\n",
        "    print(\"✗ Training results file not found\")\n",
        "    results = None\n",
        "\n",
        "# 1. Load original model\n",
        "print(\"\\nLoading original model...\")\n",
        "model_original = VGG('VGG16').to(device)\n",
        "checkpoint_original = torch.load(os.path.join(CHECKPOINT_DIR, 'model_original.pth'),\n",
        "                                map_location=device)\n",
        "model_original.load_state_dict(checkpoint_original['model_state_dict'])\n",
        "total_params = sum(p.numel() for p in model_original.parameters() if p.requires_grad)\n",
        "\n",
        "loss_original, acc_original = test_model(model_original, testloader, device)\n",
        "print(f\"✓ Original model\")\n",
        "print(f\"  Parameters: {total_params:,}\")\n",
        "print(f\"  Test accuracy: {acc_original:.2f}%\")\n",
        "print(f\"  Test loss: {loss_original:.4f}\")\n",
        "\n",
        "\n",
        "# 2. Load GSRP weight-wise pruned model\n",
        "print(\"\\nLoading GSRP weight-wise pruned model...\")\n",
        "model_weightwise = VGG('VGG16').to(device)\n",
        "\n",
        "checkpoint_weightwise = torch.load(\n",
        "    os.path.join(CHECKPOINT_DIR, 'model_GSRP_weightwise.pth'),\n",
        "    map_location=device\n",
        ")\n",
        "model_weightwise.load_state_dict(checkpoint_weightwise['model_state_dict'])\n",
        "\n",
        "# Load and apply pruning masks\n",
        "masks_weightwise = torch.load(\n",
        "    os.path.join(CHECKPOINT_DIR, 'masks_GSRP_weightwise.pth'),\n",
        "    map_location=device\n",
        ")\n",
        "apply_prune_mask(model_weightwise, masks_weightwise)\n",
        "\n",
        "nonzero_params_weightwise = count_nonzero_parameters(model_weightwise)\n",
        "loss_weightwise, acc_weightwise = test_model(model_weightwise, testloader, device)\n",
        "\n",
        "print(f\"✓ GSRP weight-wise model\")\n",
        "print(f\"  Parameters: {nonzero_params_weightwise:,} ({nonzero_params_weightwise/total_params*100:.2f}%)\")\n",
        "print(f\"  Test accuracy: {acc_weightwise:.2f}%\")\n",
        "print(f\"  Test loss: {loss_weightwise:.4f}\")\n",
        "\n",
        "# 3. Load SNIP pruned model\n",
        "print(\"\\nLoading SNIP pruned model...\")\n",
        "model_snip = VGG('VGG16').to(device)\n",
        "checkpoint_snip = torch.load(os.path.join(CHECKPOINT_DIR, 'model_snip.pth'),\n",
        "                            map_location=device)\n",
        "model_snip.load_state_dict(checkpoint_snip['model_state_dict'])\n",
        "\n",
        "# Load and apply pruning masks\n",
        "masks_snip = torch.load(os.path.join(CHECKPOINT_DIR, 'masks_snip.pth'),\n",
        "                       map_location=device)\n",
        "apply_prune_mask(model_snip, masks_snip)\n",
        "\n",
        "nonzero_params_snip = count_nonzero_parameters(model_snip)\n",
        "loss_snip, acc_snip = test_model(model_snip, testloader, device)\n",
        "print(f\"✓ SNIP pruned model\")\n",
        "print(f\"  Parameters: {nonzero_params_snip:,} ({nonzero_params_snip/total_params*100:.2f}%)\")\n",
        "print(f\"  Test accuracy: {acc_snip:.2f}%\")\n",
        "print(f\"  Test loss: {loss_snip:.4f}\")\n",
        "\n",
        "# 4. Load random pruned model\n",
        "print(\"\\nLoading random pruned model...\")\n",
        "model_random = VGG('VGG16').to(device)\n",
        "checkpoint_random = torch.load(os.path.join(CHECKPOINT_DIR, 'model_random.pth'),\n",
        "                              map_location=device)\n",
        "model_random.load_state_dict(checkpoint_random['model_state_dict'])\n",
        "\n",
        "# Load and apply pruning masks\n",
        "masks_random = torch.load(os.path.join(CHECKPOINT_DIR, 'masks_random.pth'),\n",
        "                         map_location=device)\n",
        "apply_prune_mask(model_random, masks_random)\n",
        "\n",
        "nonzero_params_random = count_nonzero_parameters(model_random)\n",
        "loss_random, acc_random = test_model(model_random, testloader, device)\n",
        "print(f\"✓ Random pruned model\")\n",
        "print(f\"  Parameters: {nonzero_params_random:,} ({nonzero_params_random/total_params*100:.2f}%)\")\n",
        "print(f\"  Test accuracy: {acc_random:.2f}%\")\n",
        "print(f\"  Test loss: {loss_random:.4f}\")\n",
        "\n",
        "\n",
        "# ============================================================================\n",
        "# Visualization comparison\n",
        "# ============================================================================\n",
        "if results is not None:\n",
        "    print(\"\\n\" + \"=\" * 70)\n",
        "    print(\"Plotting training curves and comparison charts\")\n",
        "    print(\"=\" * 70)\n",
        "\n",
        "    fig, axes = plt.subplots(2, 3, figsize=(18, 10))\n",
        "\n",
        "    # Only plot these four: original / GSRP_weightwise / snip / random\n",
        "    method_keys   = ['original', 'GSRP_weightwise', 'snip', 'random']\n",
        "    method_labels = ['Original', 'GSRP Weight-wise', 'SNIP', 'Random']\n",
        "    colors        = ['#2E86AB', '#06A77D', '#A23B72', '#F18F01']\n",
        "\n",
        "    # 1. Training accuracy\n",
        "    for key, label, c in zip(method_keys, method_labels, colors):\n",
        "        axes[0, 0].plot(results[key]['train_acc'], label=label, linewidth=2.5, color=c)\n",
        "    axes[0, 0].set_xlabel('Epoch')\n",
        "    axes[0, 0].set_ylabel('Training Accuracy (%)')\n",
        "    axes[0, 0].set_title('Training Accuracy')\n",
        "    axes[0, 0].legend()\n",
        "    axes[0, 0].grid(True, alpha=0.3)\n",
        "\n",
        "    # 2. Test accuracy\n",
        "    for key, label, c in zip(method_keys, method_labels, colors):\n",
        "        axes[0, 1].plot(results[key]['test_acc'], label=label, linewidth=2.5, color=c)\n",
        "    axes[0, 1].set_xlabel('Epoch')\n",
        "    axes[0, 1].set_ylabel('Test Accuracy (%)')\n",
        "    axes[0, 1].set_title('Test Accuracy')\n",
        "    axes[0, 1].legend()\n",
        "    axes[0, 1].grid(True, alpha=0.3)\n",
        "\n",
        "    # 3. Final accuracy bar chart (use real inference results, not just last epoch)\n",
        "    models_bar = method_labels\n",
        "    accs_bar   = [acc_original, acc_weightwise, acc_snip, acc_random]\n",
        "    bars = axes[0, 2].bar(models_bar, accs_bar, color=colors,\n",
        "                          alpha=0.8, edgecolor='black', linewidth=1.5)\n",
        "    axes[0, 2].set_ylabel('Test Accuracy (%)')\n",
        "    axes[0, 2].set_title('Final Test Accuracy')\n",
        "    axes[0, 2].set_ylim([0, 100])\n",
        "    axes[0, 2].grid(True, alpha=0.3, axis='y')\n",
        "    for bar, acc in zip(bars, accs_bar):\n",
        "        h = bar.get_height()\n",
        "        axes[0, 2].text(bar.get_x() + bar.get_width()/2., h + 1,\n",
        "                        f'{acc:.2f}%', ha='center', va='bottom',\n",
        "                        fontsize=10, fontweight='bold')\n",
        "\n",
        "    # 4. Training loss\n",
        "    for key, label, c in zip(method_keys, method_labels, colors):\n",
        "        axes[1, 0].plot(results[key]['train_loss'], label=label, linewidth=2.5, color=c)\n",
        "    axes[1, 0].set_xlabel('Epoch')\n",
        "    axes[1, 0].set_ylabel('Training Loss')\n",
        "    axes[1, 0].set_title('Training Loss')\n",
        "    axes[1, 0].legend()\n",
        "    axes[1, 0].grid(True, alpha=0.3)\n",
        "\n",
        "    # 5. Test loss\n",
        "    for key, label, c in zip(method_keys, method_labels, colors):\n",
        "        axes[1, 1].plot(results[key]['test_loss'], label=label, linewidth=2.5, color=c)\n",
        "    axes[1, 1].set_xlabel('Epoch')\n",
        "    axes[1, 1].set_ylabel('Test Loss')\n",
        "    axes[1, 1].set_title('Test Loss')\n",
        "    axes[1, 1].legend()\n",
        "    axes[1, 1].grid(True, alpha=0.3)\n",
        "\n",
        "    # 6. Parameter count bar chart\n",
        "    params_bar = [total_params,\n",
        "                  nonzero_params_weightwise,\n",
        "                  nonzero_params_snip,\n",
        "                  nonzero_params_random]\n",
        "\n",
        "    bars = axes[1, 2].bar(models_bar, params_bar, color=colors,\n",
        "                          alpha=0.8, edgecolor='black', linewidth=1.5)\n",
        "    axes[1, 2].set_ylabel('Number of Parameters')\n",
        "    axes[1, 2].set_title('Parameter Count')\n",
        "    axes[1, 2].ticklabel_format(style='plain', axis='y')\n",
        "    axes[1, 2].grid(True, alpha=0.3, axis='y')\n",
        "    for bar, param in zip(bars, params_bar):\n",
        "        h = bar.get_height()\n",
        "        axes[1, 2].text(bar.get_x() + bar.get_width()/2., h,\n",
        "                        f'{param/1e6:.2f}M', ha='center', va='bottom',\n",
        "                        fontsize=10, fontweight='bold')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig('inference_comparison.png', dpi=150, bbox_inches='tight')\n",
        "    plt.show()\n",
        "\n",
        "    print(\"✓ Chart saved as 'inference_comparison.png'\")\n",
        "\n",
        "# ============================================================================\n",
        "# Detailed comparison report\n",
        "# ============================================================================\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"Detailed Comparison Report\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "print(f\"\\n{'Model':<20} {'Parameters':<20} {'Keep Rate':<15} {'Test Accuracy':<15} {'Test Loss':<15}\")\n",
        "print(\"-\" * 80)\n",
        "print(f\"{'Original':<20} {total_params:>15,}   {'100.0%':<15} {acc_original:>10.2f}%   {loss_original:>10.4f}\")\n",
        "print(f\"{'GSRP weight-wise':<20} {nonzero_params_weightwise:>15,}   \"\n",
        "      f\"{f'{nonzero_params_weightwise/total_params*100:.1f}%':<15} {acc_weightwise:>10.2f}%   {loss_weightwise:>10.4f}\")\n",
        "print(f\"{'SNIP pruning':<20} {nonzero_params_snip:>15,}   \"\n",
        "      f\"{f'{nonzero_params_snip/total_params*100:.1f}%':<15} {acc_snip:>10.2f}%   {loss_snip:>10.4f}\")\n",
        "print(f\"{'Random pruning':<20} {nonzero_params_random:>15,}   \"\n",
        "      f\"{f'{nonzero_params_random/total_params*100:.1f}%':<15} {acc_random:>10.2f}%   {loss_random:>10.4f}\")\n",
        "\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"Accuracy Differences\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "print(f\"GSRP weight-wise vs Original: {acc_weightwise - acc_original:+.2f}% \"\n",
        "      f\"({'✓ Better' if acc_weightwise > acc_original else '✗ Worse'})\")\n",
        "print(f\"SNIP vs Original: {acc_snip - acc_original:+.2f}% \"\n",
        "      f\"({'✓ Better' if acc_snip > acc_original else '✗ Worse'})\")\n",
        "print(f\"Random vs Original: {acc_random - acc_original:+.2f}% \"\n",
        "      f\"({'✓ Better' if acc_random > acc_original else '✗ Worse'})\")\n",
        "\n",
        "print(f\"GSRP weight-wise vs SNIP: {acc_weightwise - acc_snip:+.2f}% \"\n",
        "      f\"({'✓ GSRP better' if acc_weightwise > acc_snip else '✗ SNIP better'})\")\n",
        "print(f\"GSRP weight-wise vs Random: {acc_weightwise - acc_random:+.2f}% \"\n",
        "      f\"({'✓ GSRP better' if acc_weightwise > acc_random else '✗ Random better'})\")\n",
        "print(f\"SNIP vs Random: {acc_snip - acc_random:+.2f}% \"\n",
        "      f\"({'✓ SNIP better' if acc_snip > acc_random else '✗ Random better'})\")\n",
        "\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"Conclusion\")\n",
        "print(\"=\" * 80)\n",
        "print(f\"Parameter reduction (GSRP): {(1 - nonzero_params_weightwise/total_params)*100:.1f}%\")\n",
        "print(f\"Parameter reduction (SNIP):  {(1 - nonzero_params_snip/total_params)*100:.1f}%\")\n",
        "print(f\"Parameter reduction (Random):  {(1 - nonzero_params_random/total_params)*100:.1f}%\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qcaEwJPaRFT7"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
